{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366007be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment configured\n",
      "  Device: cpu\n",
      "  LLM Model: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY in environment or .env file.\")\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4.1\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Pipeline parameters\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 80\n",
    "STAGE1_K = 30\n",
    "TOP_K_RERANKED = 5\n",
    "\n",
    "print(f\"‚úì Environment configured\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  LLM Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377a8ea",
   "metadata": {},
   "source": [
    "## Initialize Reranker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b08068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Reranker model loaded\n"
     ]
    }
   ],
   "source": [
    "RERANKER_MODEL_NAME = \"BAAI/bge-reranker-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def cross_encoder_rerank(\n",
    "    query: str,\n",
    "    docs: List[Document],\n",
    "    top_k: int = TOP_K_RERANKED\n",
    ") -> List[Document]:\n",
    "    \"\"\"Rerank documents using cross-encoder.\"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    inputs = tokenizer(\n",
    "        [p[0] for p in pairs],\n",
    "        [p[1] for p in pairs],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze(-1).cpu().numpy()\n",
    "\n",
    "    ranked_idx = np.argsort(-scores)\n",
    "    top_docs = [docs[i] for i in ranked_idx[:top_k]]\n",
    "    return top_docs\n",
    "\n",
    "print(\"‚úì Reranker model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c66f4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb0cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model() -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_llm(temperature: float = 0.7) -> ChatOpenAI:\n",
    "    return ChatOpenAI(\n",
    "        model=LLM_MODEL,\n",
    "        temperature=temperature,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565b837",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6865a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data loading functions ready\n"
     ]
    }
   ],
   "source": [
    "def load_squad_subset(max_examples: int = 1000) -> Tuple[List[Document], pd.DataFrame]:\n",
    "    \"\"\"Load SQuAD dataset for testing.\"\"\"\n",
    "    ds = load_dataset(\"squad\", split=\"train[:10%]\")\n",
    "    ds = ds.shuffle(seed=42).select(range(min(max_examples, len(ds))))\n",
    "\n",
    "    contexts = []\n",
    "    qa_rows = []\n",
    "\n",
    "    for ex in ds:\n",
    "        context = ex[\"context\"]\n",
    "        q = ex[\"question\"]\n",
    "        ans_texts = ex[\"answers\"][\"text\"]\n",
    "        ans = ans_texts[0] if ans_texts else \"\"\n",
    "\n",
    "        contexts.append(context)\n",
    "        qa_rows.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"context\": context,\n",
    "            \"question\": q,\n",
    "            \"answer\": ans\n",
    "        })\n",
    "\n",
    "    unique_contexts = list({c: True for c in contexts}.keys())\n",
    "    docs = [Document(page_content=c, metadata={\"source\": f\"squad_paragraph_{i}\"})\n",
    "            for i, c in enumerate(unique_contexts)]\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_rows)\n",
    "    return docs, qa_df\n",
    "\n",
    "def load_pdf_documents(pdf_dir: str = \"../data/pdfs\") -> List[Document]:\n",
    "    \"\"\"Load PDF documents from a directory.\"\"\"\n",
    "    pdf_path = Path(pdf_dir)\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"Creating directory: {pdf_dir}\")\n",
    "        pdf_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Please add PDF files to {pdf_dir} and run again.\")\n",
    "        return []\n",
    "    \n",
    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_dir}\")\n",
    "        return []\n",
    "    \n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Loading: {pdf_file.name}\")\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs.extend(loader.load())\n",
    "    \n",
    "    print(f\"Loaded {len(docs)} pages from {len(pdf_files)} PDF files\")\n",
    "    return docs\n",
    "\n",
    "print(\"‚úì Data loading functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ed598",
   "metadata": {},
   "source": [
    "## Build Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bee482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(\n",
    "    docs: List[Document],\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> Tuple[FAISS, List[Document]]:\n",
    "    \"\"\"Build vectorstore with chunking.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    embeddings = get_embedding_model()\n",
    "    vectordb = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return vectordb, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc36c0",
   "metadata": {},
   "source": [
    "## Feedback Learning System\n",
    "\n",
    "This system learns from thumbs up/down feedback and text comments to improve responses over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffb0970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 2 feedback entries from memory\n",
      "‚úì Feedback system initialized\n",
      "  Current satisfaction rate: 50.0%\n"
     ]
    }
   ],
   "source": [
    "class ChatbotFeedbackSystem:\n",
    "    \"\"\"Learns from user feedback to improve chatbot responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, save_path: str = \"../data/chatbot_feedback.json\"):\n",
    "        self.save_path = save_path\n",
    "        self.feedback_history: List[Dict[str, Any]] = []\n",
    "        self.positive_feedback: List[Dict[str, Any]] = []\n",
    "        self.negative_feedback: List[Dict[str, Any]] = []\n",
    "        self.improvement_keywords: Dict[str, int] = {}\n",
    "        self.load_feedback()\n",
    "    \n",
    "    def add_feedback(\n",
    "        self,\n",
    "        query: str,\n",
    "        response: str,\n",
    "        rating: str,  # \"üëç\" or \"üëé\"\n",
    "        comment: Optional[str] = None,\n",
    "        context_used: str = \"\"\n",
    "    ):\n",
    "        \"\"\"Record user feedback on a response.\"\"\"\n",
    "        feedback_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"rating\": rating,\n",
    "            \"comment\": comment,\n",
    "            \"response_length\": len(response.split()),\n",
    "            \"context_length\": len(context_used.split()) if context_used else 0,\n",
    "            \"session_number\": len(self.feedback_history) + 1\n",
    "        }\n",
    "        \n",
    "        self.feedback_history.append(feedback_entry)\n",
    "        \n",
    "        # Categorize feedback\n",
    "        if rating == \"üëç\":\n",
    "            self.positive_feedback.append(feedback_entry)\n",
    "        else:\n",
    "            self.negative_feedback.append(feedback_entry)\n",
    "            # Extract improvement keywords from negative feedback\n",
    "            if comment:\n",
    "                self._extract_improvement_keywords(comment)\n",
    "        \n",
    "        self.save_feedback()\n",
    "    \n",
    "    def _extract_improvement_keywords(self, comment: str):\n",
    "        \"\"\"Extract keywords from negative feedback to identify improvement areas.\"\"\"\n",
    "        # Common improvement indicators\n",
    "        improvement_indicators = [\n",
    "            \"more detail\", \"too long\", \"too short\", \"unclear\", \"confusing\",\n",
    "            \"not relevant\", \"missing\", \"incorrect\", \"better explanation\",\n",
    "            \"more examples\", \"simpler\", \"more technical\", \"more context\",\n",
    "            \"incomplete\", \"off-topic\", \"vague\", \"specific\", \"concise\"\n",
    "        ]\n",
    "        \n",
    "        comment_lower = comment.lower()\n",
    "        for indicator in improvement_indicators:\n",
    "            if indicator in comment_lower:\n",
    "                self.improvement_keywords[indicator] = self.improvement_keywords.get(indicator, 0) + 1\n",
    "    \n",
    "    def get_satisfaction_rate(self) -> float:\n",
    "        \"\"\"Calculate overall satisfaction rate.\"\"\"\n",
    "        if not self.feedback_history:\n",
    "            return 0.0\n",
    "        positive_count = len(self.positive_feedback)\n",
    "        return (positive_count / len(self.feedback_history)) * 100\n",
    "    \n",
    "    def get_improvement_insights(self) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze feedback to identify improvement patterns.\"\"\"\n",
    "        if not self.feedback_history:\n",
    "            return {\"total_feedback\": 0, \"insights\": \"No feedback yet\"}\n",
    "        \n",
    "        positive_avg_length = np.mean([f[\"response_length\"] for f in self.positive_feedback]) if self.positive_feedback else 0\n",
    "        negative_avg_length = np.mean([f[\"response_length\"] for f in self.negative_feedback]) if self.negative_feedback else 0\n",
    "        \n",
    "        # Identify trending issues\n",
    "        top_issues = sorted(self.improvement_keywords.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "        return {\n",
    "            \"total_feedback\": len(self.feedback_history),\n",
    "            \"positive_count\": len(self.positive_feedback),\n",
    "            \"negative_count\": len(self.negative_feedback),\n",
    "            \"satisfaction_rate\": self.get_satisfaction_rate(),\n",
    "            \"positive_avg_length\": int(positive_avg_length),\n",
    "            \"negative_avg_length\": int(negative_avg_length),\n",
    "            \"top_issues\": top_issues\n",
    "        }\n",
    "    \n",
    "    def generate_system_prompt(self) -> str:\n",
    "        \"\"\"Generate adaptive system prompt based on learned feedback.\"\"\"\n",
    "        base_prompt = \"You are a helpful AI assistant. Answer questions accurately based on the provided context.\"\n",
    "        \n",
    "        if len(self.feedback_history) < 3:\n",
    "            return base_prompt\n",
    "        \n",
    "        insights = self.get_improvement_insights()\n",
    "        adaptations = []\n",
    "        \n",
    "        # Adapt based on length preferences\n",
    "        if insights[\"positive_avg_length\"] > 0 and insights[\"negative_avg_length\"] > 0:\n",
    "            if insights[\"positive_avg_length\"] < insights[\"negative_avg_length\"]:\n",
    "                adaptations.append(\"Keep responses concise and to the point.\")\n",
    "            else:\n",
    "                adaptations.append(\"Provide detailed, comprehensive answers.\")\n",
    "        \n",
    "        # Adapt based on common issues\n",
    "        top_issues = insights.get(\"top_issues\", [])\n",
    "        for issue, count in top_issues:\n",
    "            if \"too long\" in issue:\n",
    "                adaptations.append(\"Be concise without unnecessary elaboration.\")\n",
    "            elif \"more detail\" in issue or \"incomplete\" in issue:\n",
    "                adaptations.append(\"Provide thorough explanations with sufficient detail.\")\n",
    "            elif \"unclear\" in issue or \"confusing\" in issue:\n",
    "                adaptations.append(\"Use clear, simple language and structure your answers well.\")\n",
    "            elif \"not relevant\" in issue or \"off-topic\" in issue:\n",
    "                adaptations.append(\"Focus strictly on the question asked using only the relevant context.\")\n",
    "            elif \"more examples\" in issue:\n",
    "                adaptations.append(\"Include concrete examples when helpful.\")\n",
    "            elif \"more context\" in issue:\n",
    "                adaptations.append(\"Provide background information when necessary.\")\n",
    "        \n",
    "        if adaptations:\n",
    "            return base_prompt + \" \" + \" \".join(adaptations)\n",
    "        \n",
    "        return base_prompt\n",
    "    \n",
    "    def get_recent_improvement_trend(self, window_size: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"Calculate satisfaction trend over recent interactions.\"\"\"\n",
    "        if len(self.feedback_history) < window_size:\n",
    "            return {\"trend\": \"insufficient_data\", \"recent_rate\": 0.0, \"previous_rate\": 0.0}\n",
    "        \n",
    "        # Recent window\n",
    "        recent = self.feedback_history[-window_size:]\n",
    "        recent_positive = sum(1 for f in recent if f[\"rating\"] == \"üëç\")\n",
    "        recent_rate = (recent_positive / len(recent)) * 100\n",
    "        \n",
    "        # Previous window\n",
    "        previous = self.feedback_history[-window_size*2:-window_size]\n",
    "        if len(previous) >= window_size:\n",
    "            previous_positive = sum(1 for f in previous if f[\"rating\"] == \"üëç\")\n",
    "            previous_rate = (previous_positive / len(previous)) * 100\n",
    "            \n",
    "            if recent_rate > previous_rate:\n",
    "                trend = \"improving\"\n",
    "            elif recent_rate < previous_rate:\n",
    "                trend = \"declining\"\n",
    "            else:\n",
    "                trend = \"stable\"\n",
    "        else:\n",
    "            previous_rate = 0.0\n",
    "            trend = \"insufficient_data\"\n",
    "        \n",
    "        return {\n",
    "            \"trend\": trend,\n",
    "            \"recent_rate\": recent_rate,\n",
    "            \"previous_rate\": previous_rate,\n",
    "            \"improvement\": recent_rate - previous_rate\n",
    "        }\n",
    "    \n",
    "    def save_feedback(self):\n",
    "        \"\"\"Save feedback to disk.\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)\n",
    "        with open(self.save_path, 'w') as f:\n",
    "            json.dump({\n",
    "                \"feedback_history\": self.feedback_history,\n",
    "                \"improvement_keywords\": self.improvement_keywords\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    def load_feedback(self):\n",
    "        \"\"\"Load feedback from disk.\"\"\"\n",
    "        if os.path.exists(self.save_path):\n",
    "            with open(self.save_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.feedback_history = data.get(\"feedback_history\", [])\n",
    "                self.improvement_keywords = data.get(\"improvement_keywords\", {})\n",
    "                \n",
    "                # Categorize loaded feedback\n",
    "                for entry in self.feedback_history:\n",
    "                    if entry[\"rating\"] == \"üëç\":\n",
    "                        self.positive_feedback.append(entry)\n",
    "                    else:\n",
    "                        self.negative_feedback.append(entry)\n",
    "                \n",
    "                print(f\"‚úì Loaded {len(self.feedback_history)} feedback entries from memory\")\n",
    "\n",
    "# Initialize feedback system\n",
    "feedback_system = ChatbotFeedbackSystem()\n",
    "print(f\"‚úì Feedback system initialized\")\n",
    "if feedback_system.feedback_history:\n",
    "    print(f\"  Current satisfaction rate: {feedback_system.get_satisfaction_rate():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f611a4",
   "metadata": {},
   "source": [
    "## RAG Pipeline with Adaptive Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0f09a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG pipeline ready\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(\n",
    "    vectordb: FAISS,\n",
    "    query: str,\n",
    "    stage1_k: int = STAGE1_K,\n",
    "    top_k: int = TOP_K_RERANKED\n",
    ") -> List[Document]:\n",
    "    \"\"\"Retrieve and rerank documents.\"\"\"\n",
    "    candidates = vectordb.similarity_search(query, k=stage1_k)\n",
    "    top_docs = cross_encoder_rerank(query, candidates, top_k=top_k)\n",
    "    return top_docs\n",
    "\n",
    "def generate_response(\n",
    "    query: str,\n",
    "    context_docs: List[Document],\n",
    "    feedback_system: ChatbotFeedbackSystem,\n",
    "    temperature: float = 0.7\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Generate response using learned preferences.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([d.page_content for d in context_docs])\n",
    "    \n",
    "    # Get adaptive system prompt based on feedback\n",
    "    system_prompt = feedback_system.generate_system_prompt()\n",
    "    \n",
    "    # Generate response\n",
    "    llm = get_llm(temperature=temperature)\n",
    "    \n",
    "    prompt = f\"\"\"Context:\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages).content.strip()\n",
    "    \n",
    "    return response, context\n",
    "\n",
    "print(\"‚úì RAG pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd578b07",
   "metadata": {},
   "source": [
    "## Load Data and Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02b8aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading data from: squad\n",
      "‚úì Loaded 496 documents from SQuAD\n",
      "\n",
      "üî® Building vectorstore...\n",
      "‚úì Loaded 496 documents from SQuAD\n",
      "\n",
      "üî® Building vectorstore...\n",
      "‚úì Vectorstore built with 1272 chunks\n",
      "‚úì System ready for queries\n",
      "‚úì Vectorstore built with 1272 chunks\n",
      "‚úì System ready for queries\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Choose data source\n",
    "DATA_SOURCE = \"squad\"  # Options: \"squad\" or \"pdf\"\n",
    "PDF_DIRECTORY = \"../data/pdfs\"\n",
    "\n",
    "print(f\"üìö Loading data from: {DATA_SOURCE}\")\n",
    "\n",
    "if DATA_SOURCE == \"squad\":\n",
    "    base_docs, qa_df = load_squad_subset(max_examples=600)\n",
    "    print(f\"‚úì Loaded {len(base_docs)} documents from SQuAD\")\n",
    "elif DATA_SOURCE == \"pdf\":\n",
    "    base_docs = load_pdf_documents(PDF_DIRECTORY)\n",
    "    if not base_docs:\n",
    "        print(\"\\n‚ö†Ô∏è  No PDF documents found. Please add PDFs to the directory.\")\n",
    "else:\n",
    "    raise ValueError(\"DATA_SOURCE must be 'squad' or 'pdf'\")\n",
    "\n",
    "# Build vectorstore\n",
    "if base_docs:\n",
    "    print(\"\\nüî® Building vectorstore...\")\n",
    "    vectordb, chunks = build_vectorstore(base_docs)\n",
    "    print(f\"‚úì Vectorstore built with {len(chunks)} chunks\")\n",
    "    print(f\"‚úì System ready for queries\")\n",
    "else:\n",
    "    vectordb = None\n",
    "    print(\"‚ö†Ô∏è  No documents to index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640b5b58",
   "metadata": {},
   "source": [
    "## Gradio Interface\n",
    "\n",
    "Interactive chatbot UI with feedback collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3d37a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Gradio available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1w/cdvfx941205597f7k7hc85xr0000gn/T/ipykernel_9759/4000324123.py:102: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ Gradio interface ready!\n",
      "================================================================================\n",
      "\n",
      "Run the cell below to launch the interface\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import gradio as gr\n",
    "    GRADIO_AVAILABLE = True\n",
    "    print(\"‚úì Gradio available\")\n",
    "except ImportError:\n",
    "    GRADIO_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Gradio not installed. Install with: pip install gradio\")\n",
    "    print(\"   Continuing without Gradio interface...\")\n",
    "\n",
    "if GRADIO_AVAILABLE and vectordb is not None:\n",
    "    \n",
    "    # State management for current response\n",
    "    current_query = {\"query\": \"\", \"response\": \"\", \"context\": \"\"}\n",
    "    \n",
    "    def chatbot_respond(query: str, chat_history: List[Tuple[str, str]]) -> Tuple[List[Tuple[str, str]], str]:\n",
    "        \"\"\"Generate response for chatbot.\"\"\"\n",
    "        if not query.strip():\n",
    "            return chat_history, \"\"\n",
    "        \n",
    "        # Retrieve and generate\n",
    "        top_docs = retrieve_documents(vectordb, query)\n",
    "        response, context = generate_response(query, top_docs, feedback_system)\n",
    "        \n",
    "        # Store for feedback\n",
    "        current_query[\"query\"] = query\n",
    "        current_query[\"response\"] = response\n",
    "        current_query[\"context\"] = context\n",
    "        \n",
    "        # Update chat history\n",
    "        chat_history.append((query, response))\n",
    "        \n",
    "        return chat_history, \"\"\n",
    "    \n",
    "    def thumbs_up(chat_history: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"Record positive feedback.\"\"\"\n",
    "        if current_query[\"response\"]:\n",
    "            feedback_system.add_feedback(\n",
    "                query=current_query[\"query\"],\n",
    "                response=current_query[\"response\"],\n",
    "                rating=\"üëç\",\n",
    "                context_used=current_query[\"context\"]\n",
    "            )\n",
    "            return \"‚úì Positive feedback recorded! The system will learn from this.\"\n",
    "        return \"No response to rate.\"\n",
    "    \n",
    "    def thumbs_down(chat_history: List[Tuple[str, str]], comment: str) -> str:\n",
    "        \"\"\"Record negative feedback with optional comment.\"\"\"\n",
    "        if current_query[\"response\"]:\n",
    "            feedback_system.add_feedback(\n",
    "                query=current_query[\"query\"],\n",
    "                response=current_query[\"response\"],\n",
    "                rating=\"üëé\",\n",
    "                comment=comment if comment.strip() else None,\n",
    "                context_used=current_query[\"context\"]\n",
    "            )\n",
    "            \n",
    "            insights = feedback_system.get_improvement_insights()\n",
    "            return f\"‚úì Feedback recorded! Total: {insights['total_feedback']} | Satisfaction: {insights['satisfaction_rate']:.1f}%\\nThe system will improve based on your feedback.\"\n",
    "        return \"No response to rate.\"\n",
    "    \n",
    "    def get_stats() -> str:\n",
    "        \"\"\"Get feedback statistics.\"\"\"\n",
    "        insights = feedback_system.get_improvement_insights()\n",
    "        \n",
    "        if insights[\"total_feedback\"] == 0:\n",
    "            return \"No feedback collected yet. Start chatting and provide ratings!\"\n",
    "        \n",
    "        trend = feedback_system.get_recent_improvement_trend()\n",
    "        \n",
    "        stats = f\"\"\"üìä **Feedback Statistics**\n",
    "        \n",
    "**Overall Performance:**\n",
    "- Total Interactions: {insights['total_feedback']}\n",
    "- üëç Positive: {insights['positive_count']} ({insights['satisfaction_rate']:.1f}%)\n",
    "- üëé Negative: {insights['negative_count']}\n",
    "\n",
    "**Recent Trend (Last 5):**\n",
    "- Trend: {trend['trend'].upper()}\n",
    "- Recent Satisfaction: {trend['recent_rate']:.1f}%\n",
    "- Previous Satisfaction: {trend['previous_rate']:.1f}%\n",
    "- Change: {trend.get('improvement', 0.0):+.1f}%\n",
    "\n",
    "**Response Characteristics:**\n",
    "- Positive Avg Length: {insights['positive_avg_length']} words\n",
    "- Negative Avg Length: {insights['negative_avg_length']} words\n",
    "\"\"\"\n",
    "        \n",
    "        if insights.get(\"top_issues\"):\n",
    "            stats += \"\\n**Top Improvement Areas:**\\n\"\n",
    "            for issue, count in insights[\"top_issues\"]:\n",
    "                stats += f\"- {issue}: {count} mentions\\n\"\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    # Create Gradio interface\n",
    "    with gr.Blocks(title=\"RAG Chatbot with Feedback Learning\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"# ü§ñ RAG Chatbot with Feedback Learning\")\n",
    "        gr.Markdown(\"Ask questions and provide feedback to help the system improve!\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Chat History\",\n",
    "                    height=400,\n",
    "                    show_copy_button=True\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    query_input = gr.Textbox(\n",
    "                        label=\"Your Question\",\n",
    "                        placeholder=\"Ask me anything...\",\n",
    "                        lines=2,\n",
    "                        scale=4\n",
    "                    )\n",
    "                    submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                \n",
    "                gr.Markdown(\"### üí¨ Rate the Response\")\n",
    "                with gr.Row():\n",
    "                    thumbs_up_btn = gr.Button(\"üëç Good Response\", variant=\"primary\", scale=1)\n",
    "                    thumbs_down_btn = gr.Button(\"üëé Needs Improvement\", variant=\"stop\", scale=1)\n",
    "                \n",
    "                feedback_comment = gr.Textbox(\n",
    "                    label=\"Feedback Comment (optional - helps system learn)\",\n",
    "                    placeholder=\"What could be improved? (e.g., 'too long', 'more detail needed', 'unclear')\",\n",
    "                    lines=2\n",
    "                )\n",
    "                \n",
    "                feedback_output = gr.Textbox(label=\"Feedback Status\", lines=2)\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### üìà Learning Progress\")\n",
    "                stats_output = gr.Markdown(get_stats())\n",
    "                refresh_stats_btn = gr.Button(\"üîÑ Refresh Stats\", variant=\"secondary\")\n",
    "                \n",
    "                gr.Markdown(\"### üí° Current System Prompt\")\n",
    "                system_prompt_display = gr.Textbox(\n",
    "                    value=feedback_system.generate_system_prompt(),\n",
    "                    label=\"Adaptive Prompt\",\n",
    "                    lines=6,\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        # Example queries\n",
    "        gr.Markdown(\"### üìù Example Questions\")\n",
    "        example_queries = [\n",
    "            \"What is the capital of France?\",\n",
    "            \"Who invented the telephone?\",\n",
    "            \"When did World War II end?\",\n",
    "            \"What is photosynthesis?\"\n",
    "        ]\n",
    "        gr.Examples(examples=example_queries, inputs=query_input)\n",
    "        \n",
    "        # Event handlers\n",
    "        submit_btn.click(\n",
    "            fn=chatbot_respond,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[chatbot, query_input]\n",
    "        ).then(\n",
    "            fn=lambda: feedback_system.generate_system_prompt(),\n",
    "            outputs=system_prompt_display\n",
    "        )\n",
    "        \n",
    "        query_input.submit(\n",
    "            fn=chatbot_respond,\n",
    "            inputs=[query_input, chatbot],\n",
    "            outputs=[chatbot, query_input]\n",
    "        ).then(\n",
    "            fn=lambda: feedback_system.generate_system_prompt(),\n",
    "            outputs=system_prompt_display\n",
    "        )\n",
    "        \n",
    "        thumbs_up_btn.click(\n",
    "            fn=thumbs_up,\n",
    "            inputs=[chatbot],\n",
    "            outputs=feedback_output\n",
    "        ).then(\n",
    "            fn=get_stats,\n",
    "            outputs=stats_output\n",
    "        ).then(\n",
    "            fn=lambda: feedback_system.generate_system_prompt(),\n",
    "            outputs=system_prompt_display\n",
    "        )\n",
    "        \n",
    "        thumbs_down_btn.click(\n",
    "            fn=thumbs_down,\n",
    "            inputs=[chatbot, feedback_comment],\n",
    "            outputs=feedback_output\n",
    "        ).then(\n",
    "            fn=get_stats,\n",
    "            outputs=stats_output\n",
    "        ).then(\n",
    "            fn=lambda: \"\",\n",
    "            outputs=feedback_comment\n",
    "        ).then(\n",
    "            fn=lambda: feedback_system.generate_system_prompt(),\n",
    "            outputs=system_prompt_display\n",
    "        )\n",
    "        \n",
    "        refresh_stats_btn.click(\n",
    "            fn=get_stats,\n",
    "            outputs=stats_output\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ Gradio interface ready!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nRun the cell below to launch the interface\")\n",
    "\n",
    "else:\n",
    "    if not GRADIO_AVAILABLE:\n",
    "        print(\"\\n‚ö†Ô∏è  Gradio not available. Install with: pip install gradio\")\n",
    "    if vectordb is None:\n",
    "        print(\"\\n‚ö†Ô∏è  No vectorstore available. Load data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42983e",
   "metadata": {},
   "source": [
    "## Launch Gradio Interface\n",
    "\n",
    "Run this cell to start the interactive chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64505231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/gradio/queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/gradio/blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/gradio/utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/1w/cdvfx941205597f7k7hc85xr0000gn/T/ipykernel_9759/4000324123.py\", line 21, in chatbot_respond\n",
      "    top_docs = retrieve_documents(vectordb, query)\n",
      "  File \"/var/folders/1w/cdvfx941205597f7k7hc85xr0000gn/T/ipykernel_9759/4236438040.py\", line 8, in retrieve_documents\n",
      "    candidates = vectordb.similarity_search(query, k=stage1_k)\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\", line 643, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\", line 515, in similarity_search_with_score\n",
      "    embedding = self._embed_query(query)\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\", line 266, in _embed_query\n",
      "    return self.embedding_function.embed_query(text)\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/langchain_openai/embeddings/base.py\", line 752, in embed_query\n",
      "    return self.embed_documents([text], **kwargs)[0]\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/langchain_openai/embeddings/base.py\", line 702, in embed_documents\n",
      "    return self._get_len_safe_embeddings(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/langchain_openai/embeddings/base.py\", line 569, in _get_len_safe_embeddings\n",
      "    response = self.client.create(input=batch_tokens, **client_kwargs)\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/openai/resources/embeddings.py\", line 132, in create\n",
      "    return self._post(\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "if GRADIO_AVAILABLE and vectordb is not None:\n",
    "    # Launch the interface\n",
    "    demo.launch(\n",
    "        share=False,  # Set to True to create a public link\n",
    "        server_name=\"127.0.0.1\",\n",
    "        server_port=7861,\n",
    "        show_error=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Cannot launch Gradio interface. Check previous cells for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc16b16",
   "metadata": {},
   "source": [
    "## Command Line Testing (Alternative to Gradio)\n",
    "\n",
    "Test the chatbot without Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chatbot_cli(vectordb: FAISS, feedback_system: ChatbotFeedbackSystem, query: str):\n",
    "    \"\"\"Test chatbot in notebook.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üîç QUERY: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Retrieve and generate\n",
    "    top_docs = retrieve_documents(vectordb, query)\n",
    "    print(f\"‚úì Retrieved {len(top_docs)} relevant documents\")\n",
    "    \n",
    "    response, context = generate_response(query, top_docs, feedback_system)\n",
    "    \n",
    "    print(f\"\\nüìù RESPONSE:\")\n",
    "    print(\"-\"*80)\n",
    "    print(response)\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Length: {len(response.split())} words\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return response, context\n",
    "\n",
    "# Example test\n",
    "if vectordb is not None:\n",
    "    test_query = \"What is the capital of France?\"\n",
    "    test_response, test_context = test_chatbot_cli(vectordb, feedback_system, test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38743364",
   "metadata": {},
   "source": [
    "## Manual Feedback Recording (For CLI Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87eaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record feedback for the test query above\n",
    "# Uncomment and modify to record feedback:\n",
    "\n",
    "# feedback_system.add_feedback(\n",
    "#     query=test_query,\n",
    "#     response=test_response,\n",
    "#     rating=\"üëç\",  # or \"üëé\"\n",
    "#     comment=\"Good response, very clear\",  # Optional\n",
    "#     context_used=test_context\n",
    "# )\n",
    "\n",
    "# print(\"‚úì Feedback recorded\")\n",
    "# print(f\"Current satisfaction rate: {feedback_system.get_satisfaction_rate():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ccbdb",
   "metadata": {},
   "source": [
    "## Feedback Analysis and Improvement Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03de008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feedback_improvement():\n",
    "    \"\"\"Visualize feedback trends and improvements.\"\"\"\n",
    "    if not feedback_system.feedback_history:\n",
    "        print(\"No feedback data to visualize yet.\")\n",
    "        return\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    df = pd.DataFrame(feedback_system.feedback_history)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Chatbot Feedback Analysis & Improvement Tracking', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Cumulative satisfaction rate over time\n",
    "    df['positive'] = (df['rating'] == 'üëç').astype(int)\n",
    "    df['cumulative_satisfaction'] = df['positive'].expanding().mean() * 100\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(df['session_number'], df['cumulative_satisfaction'], marker='o', linewidth=2, markersize=6)\n",
    "    ax1.axhline(y=50, color='r', linestyle='--', alpha=0.3, label='50% baseline')\n",
    "    ax1.set_xlabel('Session Number')\n",
    "    ax1.set_ylabel('Satisfaction Rate (%)')\n",
    "    ax1.set_title('Cumulative Satisfaction Rate Over Time')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim([0, 100])\n",
    "    \n",
    "    # 2. Rolling satisfaction rate (window=5)\n",
    "    if len(df) >= 5:\n",
    "        df['rolling_satisfaction'] = df['positive'].rolling(window=5, min_periods=1).mean() * 100\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.plot(df['session_number'], df['rolling_satisfaction'], marker='s', linewidth=2, markersize=6, color='green')\n",
    "        ax2.axhline(y=50, color='r', linestyle='--', alpha=0.3, label='50% baseline')\n",
    "        ax2.set_xlabel('Session Number')\n",
    "        ax2.set_ylabel('Satisfaction Rate (%)')\n",
    "        ax2.set_title('Rolling Satisfaction Rate (Window=5)')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        ax2.set_ylim([0, 100])\n",
    "    else:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Need 5+ sessions for rolling average', \n",
    "                       ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Rolling Satisfaction Rate (Window=5)')\n",
    "    \n",
    "    # 3. Rating distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    rating_counts = df['rating'].value_counts()\n",
    "    colors = ['#4CAF50' if r == 'üëç' else '#F44336' for r in rating_counts.index]\n",
    "    ax3.bar(rating_counts.index, rating_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xlabel('Rating')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.set_title('Rating Distribution')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    total = len(df)\n",
    "    for i, (rating, count) in enumerate(rating_counts.items()):\n",
    "        percentage = (count / total) * 100\n",
    "        ax3.text(i, count, f'{count}\\n({percentage:.1f}%)', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Response length comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    positive_lengths = df[df['rating'] == 'üëç']['response_length']\n",
    "    negative_lengths = df[df['rating'] == 'üëé']['response_length']\n",
    "    \n",
    "    if len(positive_lengths) > 0 and len(negative_lengths) > 0:\n",
    "        ax4.boxplot([positive_lengths, negative_lengths], labels=['üëç Positive', 'üëé Negative'])\n",
    "        ax4.set_ylabel('Response Length (words)')\n",
    "        ax4.set_title('Response Length by Rating')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Need both positive and negative feedback', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Response Length by Rating')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed insights\n",
    "    insights = feedback_system.get_improvement_insights()\n",
    "    trend = feedback_system.get_recent_improvement_trend()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä DETAILED FEEDBACK INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìà Overall Performance:\")\n",
    "    print(f\"   Total Feedback: {insights['total_feedback']}\")\n",
    "    print(f\"   üëç Positive: {insights['positive_count']} ({insights['satisfaction_rate']:.1f}%)\")\n",
    "    print(f\"   üëé Negative: {insights['negative_count']}\")\n",
    "    \n",
    "    print(f\"\\nüìä Recent Trend (Last 5 sessions):\")\n",
    "    print(f\"   Status: {trend['trend'].upper()}\")\n",
    "    print(f\"   Recent Rate: {trend['recent_rate']:.1f}%\")\n",
    "    print(f\"   Previous Rate: {trend['previous_rate']:.1f}%\")\n",
    "    print(f\"   Change: {trend['improvement'], 0.0:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìè Response Characteristics:\")\n",
    "    print(f\"   Positive Responses Avg: {insights['positive_avg_length']} words\")\n",
    "    print(f\"   Negative Responses Avg: {insights['negative_avg_length']} words\")\n",
    "    \n",
    "    if insights.get(\"top_issues\"):\n",
    "        print(f\"\\n‚ö†Ô∏è  Top Improvement Areas:\")\n",
    "        for issue, count in insights[\"top_issues\"]:\n",
    "            print(f\"   - {issue}: {count} mentions\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run visualization\n",
    "if feedback_system.feedback_history:\n",
    "    visualize_feedback_improvement()\n",
    "else:\n",
    "    print(\"üìä No feedback data yet. Use the chatbot and provide feedback first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16242da0",
   "metadata": {},
   "source": [
    "## Export Feedback Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_feedback_data():\n",
    "    \"\"\"Export feedback to CSV for analysis.\"\"\"\n",
    "    if not feedback_system.feedback_history:\n",
    "        print(\"No feedback to export\")\n",
    "        return\n",
    "    \n",
    "    output_path = \"../data/chatbot_feedback_export.csv\"\n",
    "    df = pd.DataFrame(feedback_system.feedback_history)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"‚úì Feedback data exported to: {output_path}\")\n",
    "    print(f\"  Total entries: {len(df)}\")\n",
    "    print(f\"  Satisfaction rate: {feedback_system.get_satisfaction_rate():.1f}%\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nüìÑ Sample of exported data:\")\n",
    "    print(df[['timestamp', 'query', 'rating', 'response_length', 'comment']].tail())\n",
    "\n",
    "# Uncomment to export:\n",
    "# export_feedback_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c001caa",
   "metadata": {},
   "source": [
    "## View Current System Adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_system_adaptations():\n",
    "    \"\"\"Display how the system has adapted based on feedback.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß† SYSTEM ADAPTATIONS BASED ON FEEDBACK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìã Current System Prompt:\")\n",
    "    print(\"-\"*80)\n",
    "    current_prompt = feedback_system.generate_system_prompt()\n",
    "    print(current_prompt)\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    insights = feedback_system.get_improvement_insights()\n",
    "    \n",
    "    if insights[\"total_feedback\"] < 3:\n",
    "        print(\"\\n‚è≥ Not enough feedback yet to show significant adaptations.\")\n",
    "        print(\"   Provide more feedback (at least 3) to see system improvements!\")\n",
    "    else:\n",
    "        print(\"\\nüéØ Adaptation Details:\")\n",
    "        \n",
    "        if insights[\"positive_avg_length\"] > 0 and insights[\"negative_avg_length\"] > 0:\n",
    "            if insights[\"positive_avg_length\"] < insights[\"negative_avg_length\"]:\n",
    "                print(\"   ‚úì Learned to prefer CONCISE responses\")\n",
    "                print(f\"     (Positive avg: {insights['positive_avg_length']} words, Negative avg: {insights['negative_avg_length']} words)\")\n",
    "            else:\n",
    "                print(\"   ‚úì Learned to prefer DETAILED responses\")\n",
    "                print(f\"     (Positive avg: {insights['positive_avg_length']} words, Negative avg: {insights['negative_avg_length']} words)\")\n",
    "        \n",
    "        if insights.get(\"top_issues\"):\n",
    "            print(\"\\n   ‚úì Addressing these issues:\")\n",
    "            for issue, count in insights[\"top_issues\"]:\n",
    "                print(f\"     - {issue} ({count} mentions)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "show_system_adaptations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8bf32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements:\n",
    "\n",
    "‚úÖ **Single-Response Chatbot**: One response per query (no A/B testing)\n",
    "‚úÖ **Thumbs Up/Down Feedback**: Simple rating system  \n",
    "‚úÖ **Optional Text Feedback**: Users can explain what needs improvement\n",
    "‚úÖ **Adaptive Learning**: System adjusts based on feedback patterns\n",
    "‚úÖ **Gradio Interface**: Beautiful, interactive UI for chatting\n",
    "‚úÖ **Improvement Tracking**: Visualizations show satisfaction trends over time\n",
    "‚úÖ **Performance Metrics**: Real-time stats on satisfaction rate and improvements\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Feedback Learning System**: Analyzes thumbs up/down ratings and text comments\n",
    "2. **Adaptive System Prompts**: Automatically adjusts based on user preferences\n",
    "3. **Improvement Detection**: Tracks satisfaction rate trends (improving/declining/stable)\n",
    "4. **Interactive UI**: Gradio interface with chat history and real-time stats\n",
    "5. **Visualization**: Charts showing cumulative satisfaction, rolling trends, and response characteristics\n",
    "\n",
    "### How to Use:\n",
    "\n",
    "1. **With Gradio**: Launch the interface and chat naturally with feedback buttons\n",
    "2. **Without Gradio**: Use CLI testing functions and manually record feedback\n",
    "3. **Track Progress**: Run visualization cell to see improvement over time\n",
    "\n",
    "The system learns from your feedback and adapts its responses to match your preferences! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Advanced-RAG-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
