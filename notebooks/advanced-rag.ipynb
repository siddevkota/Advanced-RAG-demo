{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cdb163",
   "metadata": {},
   "source": [
    "# Advanced RAG Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01370dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY in environment or .env file.\")\n",
    "\n",
    "# Global config\n",
    "DATA_MODE = \"squad\"  # \"squad\" or \"uploaded\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\"  # change if you like\n",
    "\n",
    "# Top-level pipeline knobs\n",
    "STAGE1_K = 30\n",
    "TOP_K_RERANKED = 5\n",
    "DEFAULT_CHUNK_SIZE = 400\n",
    "DEFAULT_CHUNK_OVERLAP = 80\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6210fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-encoder reranker using bge-reranker-base\n",
    "# If this model is too heavy on your machine, you can swap it for another or use LLM-based reranking.\n",
    "\n",
    "RERANKER_MODEL_NAME = \"BAAI/bge-reranker-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def cross_encoder_rerank(\n",
    "    query: str,\n",
    "    docs: List[Document],\n",
    "    top_k: int = TOP_K_RERANKED\n",
    ") -> List[Document]:\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    inputs = tokenizer(\n",
    "        [p[0] for p in pairs],\n",
    "        [p[1] for p in pairs],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze(-1).cpu().numpy()\n",
    "\n",
    "    ranked_idx = np.argsort(-scores)  # descending\n",
    "    top_docs = [docs[i] for i in ranked_idx[:top_k]]\n",
    "    return top_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cf737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_splitter(chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "                      chunk_overlap: int = DEFAULT_CHUNK_OVERLAP) -> RecursiveCharacterTextSplitter:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    return splitter\n",
    "\n",
    "\n",
    "def get_embedding_model() -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "def get_llm(model_name: str = LLM_MODEL, temperature: float = 0.0) -> ChatOpenAI:\n",
    "    return ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd3f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(max_examples: int = 2000) -> Tuple[List[Document], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load a subset of SQuAD v1.1 and convert contexts to Documents.\n",
    "    Also returns a small dataframe of QA pairs for evaluation.\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\"squad\", split=\"train[:10%]\")  # small subset\n",
    "    ds = ds.shuffle(seed=42).select(range(min(max_examples, len(ds))))\n",
    "\n",
    "    contexts = []\n",
    "    qa_rows = []\n",
    "\n",
    "    for ex in ds:\n",
    "        context = ex[\"context\"]\n",
    "        q = ex[\"question\"]\n",
    "        ans_texts = ex[\"answers\"][\"text\"]\n",
    "        ans = ans_texts[0] if ans_texts else \"\"\n",
    "\n",
    "        contexts.append(context)\n",
    "        qa_rows.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"context\": context,\n",
    "            \"question\": q,\n",
    "            \"answer\": ans\n",
    "        })\n",
    "\n",
    "    # dedupe contexts\n",
    "    unique_contexts = list({c: True for c in contexts}.keys())\n",
    "\n",
    "    docs = [Document(page_content=c, metadata={\"source\": f\"squad_paragraph_{i}\"})\n",
    "            for i, c in enumerate(unique_contexts)]\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_rows)\n",
    "    return docs, qa_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d69a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_uploaded_docs(data_dir: str = \"./data/uploaded\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load user docs from a folder. Supports .txt, .md, .pdf out of the box.\n",
    "    You can add more loaders as needed.\n",
    "    \"\"\"\n",
    "    base = Path(data_dir)\n",
    "    if not base.exists():\n",
    "        raise ValueError(f\"{data_dir} does not exist. Create it and drop files there.\")\n",
    "\n",
    "    docs: List[Document] = []\n",
    "\n",
    "    for path in base.rglob(\"*\"):\n",
    "        if path.is_dir():\n",
    "            continue\n",
    "\n",
    "        ext = path.suffix.lower()\n",
    "\n",
    "        if ext in [\".txt\", \".md\"]:\n",
    "            loader = TextLoader(str(path), encoding=\"utf-8\")\n",
    "        elif ext == \".pdf\":\n",
    "            loader = PyPDFLoader(str(path))\n",
    "        else:\n",
    "            # ignore unsupported types for now\n",
    "            continue\n",
    "\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    if not docs:\n",
    "        raise ValueError(f\"No supported files found in {data_dir}. Add txt/md/pdf files.\")\n",
    "\n",
    "    # We don't have ground truth answers for uploaded docs; evaluation will be question-only.\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42cfd424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(\n",
    "    docs: List[Document],\n",
    "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP\n",
    ") -> Tuple[FAISS, List[Document]]:\n",
    "    splitter = get_text_splitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = get_embedding_model()\n",
    "    vectordb = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return vectordb, chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81bec633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query: str) -> str:\n",
    "    llm = get_llm()\n",
    "    system_msg = (\n",
    "        \"You are a helpful assistant that rewrites user queries for better retrieval. \"\n",
    "        \"Expand abbreviations and add key synonyms, but keep the core information need unchanged. \"\n",
    "        \"Return ONLY the rewritten query text, no explanations.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    resp = llm.invoke(messages)\n",
    "    return resp.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406a04d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage1_retrieve(\n",
    "    vectordb: FAISS,\n",
    "    query: str,\n",
    "    k: int = STAGE1_K\n",
    ") -> List[Document]:\n",
    "    docs = vectordb.similarity_search(query, k=k)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a584c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(\n",
    "    query: str,\n",
    "    candidates: List[Document],\n",
    "    top_k: int = TOP_K_RERANKED\n",
    ") -> List[Document]:\n",
    "    if not candidates:\n",
    "        return []\n",
    "    return cross_encoder_rerank(query, candidates, top_k=top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513d67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_context(\n",
    "    query: str,\n",
    "    docs: List[Document],\n",
    "    max_tokens_hint: int = 800\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Abstractive compression: produce a concise, focused brief\n",
    "    that keeps only information needed to answer the query.\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return \"\"\n",
    "\n",
    "    llm = get_llm()\n",
    "\n",
    "    joined = \"\\n\\n\".join(\n",
    "        [f\"[DOC {i} | source={d.metadata.get('source','unknown')}]\\n{d.page_content}\"\n",
    "         for i, d in enumerate(docs)]\n",
    "    )\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are a careful information compressor. Given a user query and multiple text chunks, \"\n",
    "        \"you produce a concise but information-rich summary that retains ONLY the details relevant \"\n",
    "        \"to answering the query. Keep factual details and numbers. Avoid general commentary.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"User query:\n",
    "{query}\n",
    "\n",
    "Raw retrieved text:\n",
    "{joined}\n",
    "\n",
    "Compress the above into a single focused brief (<= {max_tokens_hint} tokens)\n",
    "containing only information directly relevant to answering the query.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    resp = llm.invoke(messages)\n",
    "    return resp.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eebaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(\n",
    "    query: str,\n",
    "    compressed_context: str\n",
    ") -> str:\n",
    "    llm = get_llm()\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are a question answering system that MUST stay grounded in the provided context. \"\n",
    "        \"Follow these rules:\\n\"\n",
    "        \"- Use ONLY the context to answer.\\n\"\n",
    "        \"- If the answer is not in the context, say you don't know.\\n\"\n",
    "        \"- When possible, quote short phrases from the context and mention which part you used.\\n\"\n",
    "        \"- Do not invent facts.\\n\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"Context to use (may be compressed):\n",
    "\\\"\\\"\\\"{compressed_context}\\\"\\\"\\\"\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\n",
    "Instructions:\n",
    "- Base your answer ONLY on the context above.\n",
    "- If the answer is unknown or not clearly supported, explicitly say so.\n",
    "- Keep the answer concise but clear.\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    resp = llm.invoke(messages)\n",
    "    return resp.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0437581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer(\n",
    "    query: str,\n",
    "    answer: str,\n",
    "    context: str,\n",
    "    ground_truth: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use an LLM-judge to score:\n",
    "    - correctness (1-5)\n",
    "    - groundedness (1-5)\n",
    "    - relevance (1-5)\n",
    "    Also return a short explanation.\n",
    "    \"\"\"\n",
    "    llm = get_llm()\n",
    "\n",
    "    system_msg = (\n",
    "        \"You are an evaluation assistant. \"\n",
    "        \"You will be given a question, a context, a model answer, and optionally a ground truth answer. \"\n",
    "        \"You must score the model's answer on:\\n\"\n",
    "        \"- correctness: is it factually correct vs ground truth/context? (1-5)\\n\"\n",
    "        \"- groundedness: is it clearly supported by the context? (1-5)\\n\"\n",
    "        \"- relevance: is it focused on the question? (1-5)\\n\"\n",
    "        \"Return a JSON object with keys: correctness, groundedness, relevance, explanation.\"\n",
    "    )\n",
    "\n",
    "    gt_text = ground_truth if ground_truth is not None else \"(no ground truth provided)\"\n",
    "\n",
    "    user_prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Ground truth answer (may be empty): {gt_text}\n",
    "\n",
    "Context:\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "Model answer:\n",
    "\\\"\\\"\\\"{answer}\\\"\\\"\\\"\n",
    "\n",
    "Now output a JSON object like:\n",
    "{{\n",
    "  \"correctness\": 1-5 integer,\n",
    "  \"groundedness\": 1-5 integer,\n",
    "  \"relevance\": 1-5 integer,\n",
    "  \"explanation\": \"short explanation here\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    resp = llm.invoke(messages)\n",
    "    raw = resp.content.strip()\n",
    "\n",
    "    # basic JSON parsing with fallback\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception:\n",
    "        # Try to extract JSON substring\n",
    "        start = raw.find(\"{\")\n",
    "        end = raw.rfind(\"}\")\n",
    "        if start != -1 and end != -1:\n",
    "            try:\n",
    "                data = json.loads(raw[start:end+1])\n",
    "            except Exception:\n",
    "                data = {}\n",
    "        else:\n",
    "            data = {}\n",
    "\n",
    "    def safe_int(key, default=3):\n",
    "        try:\n",
    "            return int(data.get(key, default))\n",
    "        except Exception:\n",
    "            return default\n",
    "\n",
    "    return {\n",
    "        \"correctness\": safe_int(\"correctness\"),\n",
    "        \"groundedness\": safe_int(\"groundedness\"),\n",
    "        \"relevance\": safe_int(\"relevance\"),\n",
    "        \"explanation\": data.get(\"explanation\", raw[:300])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0637746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackMemory:\n",
    "    def __init__(self):\n",
    "        self.docs: List[Document] = []\n",
    "        self.vectordb: Optional[FAISS] = None\n",
    "\n",
    "    def rebuild_index(self):\n",
    "        if not self.docs:\n",
    "            self.vectordb = None\n",
    "            return\n",
    "        embeddings = get_embedding_model()\n",
    "        self.vectordb = FAISS.from_documents(self.docs, embeddings)\n",
    "\n",
    "    def add_feedback(self, query: str, corrected_fact: str):\n",
    "        doc = Document(\n",
    "            page_content=corrected_fact,\n",
    "            metadata={\"source\": \"user_feedback\", \"query\": query}\n",
    "        )\n",
    "        self.docs.append(doc)\n",
    "        self.rebuild_index()\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Document]:\n",
    "        if self.vectordb is None:\n",
    "            return []\n",
    "        return self.vectordb.similarity_search(query, k=k)\n",
    "\n",
    "\n",
    "memory_store = FeedbackMemory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b977663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_memory(\n",
    "    vectordb: FAISS,\n",
    "    query: str,\n",
    "    stage1_k: int = STAGE1_K,\n",
    "    top_k_reranked: int = TOP_K_RERANKED,\n",
    ") -> List[Document]:\n",
    "    main_docs = stage1_retrieve(vectordb, query, k=stage1_k)\n",
    "    mem_docs = memory_store.retrieve(query, k=5)\n",
    "\n",
    "    combined = main_docs + mem_docs\n",
    "    reranked = rerank(query, combined, top_k=top_k_reranked)\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45e5771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_query_pipeline(\n",
    "    vectordb: FAISS,\n",
    "    query: str,\n",
    "    ground_truth: Optional[str] = None,\n",
    "    use_memory: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    timings = {}\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Rewrite\n",
    "    t_start = time.time()\n",
    "    rewritten = rewrite_query(query)\n",
    "    timings[\"rewrite_ms\"] = int((time.time() - t_start) * 1000)\n",
    "\n",
    "    # Retrieval\n",
    "    t_start = time.time()\n",
    "    if use_memory:\n",
    "        top_docs = retrieve_with_memory(vectordb, rewritten)\n",
    "    else:\n",
    "        candidates = stage1_retrieve(vectordb, rewritten, k=STAGE1_K)\n",
    "        top_docs = rerank(rewritten, candidates, top_k=TOP_K_RERANKED)\n",
    "    timings[\"retrieval_plus_rerank_ms\"] = int((time.time() - t_start) * 1000)\n",
    "\n",
    "    # Compression\n",
    "    t_start = time.time()\n",
    "    compressed = compress_context(rewritten, top_docs)\n",
    "    timings[\"compression_ms\"] = int((time.time() - t_start) * 1000)\n",
    "\n",
    "    # Generation\n",
    "    t_start = time.time()\n",
    "    answer = generate_answer(query, compressed)\n",
    "    timings[\"generation_ms\"] = int((time.time() - t_start) * 1000)\n",
    "\n",
    "    # Evaluation\n",
    "    t_start = time.time()\n",
    "    eval_result = evaluate_answer(\n",
    "        query=query,\n",
    "        answer=answer,\n",
    "        context=compressed,\n",
    "        ground_truth=ground_truth\n",
    "    )\n",
    "    timings[\"evaluation_ms\"] = int((time.time() - t_start) * 1000)\n",
    "\n",
    "    timings[\"total_ms\"] = int((time.time() - t0) * 1000)\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Original query:\", query)\n",
    "    print(\"Rewritten query:\", rewritten)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Top documents (sources):\")\n",
    "    for i, d in enumerate(top_docs):\n",
    "        print(f\"[{i}] source={d.metadata.get('source','unknown')}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Compressed context (preview):\")\n",
    "    print(compressed[:1000] + (\"...\" if len(compressed) > 1000 else \"\"))\n",
    "    print(\"-\" * 80)\n",
    "    if ground_truth is not None:\n",
    "        print(\"Ground truth answer:\", ground_truth)\n",
    "        print(\"-\" * 80)\n",
    "    print(\"Model answer:\")\n",
    "    print(answer)\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Evaluation scores:\")\n",
    "    print(f\"  correctness:  {eval_result['correctness']}\")\n",
    "    print(f\"  groundedness: {eval_result['groundedness']}\")\n",
    "    print(f\"  relevance:    {eval_result['relevance']}\")\n",
    "    print(f\"  explanation:  {eval_result['explanation']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Latency (ms):\")\n",
    "    for k, v in timings.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"rewritten_query\": rewritten,\n",
    "        \"answer\": answer,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"compressed_context\": compressed,\n",
    "        \"top_docs\": top_docs,\n",
    "        \"eval\": eval_result,\n",
    "        \"timings\": timings,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d24d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 261895.11 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 416311.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 496 SQuAD base documents and 600 QA pairs.\n",
      "Vectorstore built with 1272 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Choose which mode to run\n",
    "DATA_MODE = \"squad\"      # or \"uploaded\"\n",
    "\n",
    "if DATA_MODE == \"squad\":\n",
    "    base_docs, qa_df = load_squad_subset(max_examples=600)\n",
    "    print(f\"Loaded {len(base_docs)} SQuAD base documents and {len(qa_df)} QA pairs.\")\n",
    "    vectordb, all_chunks = build_vectorstore(base_docs, chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP)\n",
    "    print(f\"Vectorstore built with {len(all_chunks)} chunks.\")\n",
    "else:\n",
    "    base_docs = load_uploaded_docs(\"./data/uploaded\")\n",
    "    print(f\"Loaded {len(base_docs)} uploaded documents.\")\n",
    "    qa_df = None  # no ground truth\n",
    "    vectordb, all_chunks = build_vectorstore(base_docs, chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP)\n",
    "    print(f\"Vectorstore built with {len(all_chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ea5ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Original query: Who was the superior of prince Kublai?\n",
      "Rewritten query: Who was the superior or leader of Prince Kublai Khan?\n",
      "--------------------------------------------------------------------------------\n",
      "Top documents (sources):\n",
      "[0] source=squad_paragraph_105\n",
      "[1] source=squad_paragraph_105\n",
      "[2] source=squad_paragraph_105\n",
      "[3] source=squad_paragraph_129\n",
      "[4] source=squad_paragraph_170\n",
      "--------------------------------------------------------------------------------\n",
      "Compressed context (preview):\n",
      "Prince Kublai Khan's superior was Ögedei Khan, who granted Kublai a large appanage in North China starting in 1236. Kublai later ruled as Khagan from 1260 to 1294.\n",
      "--------------------------------------------------------------------------------\n",
      "Ground truth answer: Ögedei Khan\n",
      "--------------------------------------------------------------------------------\n",
      "Model answer:\n",
      "Prince Kublai Khan's superior was Ögedei Khan.\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation scores:\n",
      "  correctness:  5\n",
      "  groundedness: 5\n",
      "  relevance:    5\n",
      "  explanation:  The model answer is factually correct as it accurately identifies Ögedei Khan as Kublai's superior, which is supported by the context provided.\n",
      "--------------------------------------------------------------------------------\n",
      "Latency (ms):\n",
      "  rewrite_ms: 1218\n",
      "  retrieval_plus_rerank_ms: 6484\n",
      "  compression_ms: 1440\n",
      "  generation_ms: 1180\n",
      "  evaluation_ms: 1906\n",
      "  total_ms: 12230\n",
      "================================================================================\n",
      "================================================================================\n",
      "Original query: In turn, Chinese supporters have accused Western media of being what in their coverage?\n",
      "Rewritten query: In response, Chinese supporters have accused Western media of being biased in their coverage.\n",
      "--------------------------------------------------------------------------------\n",
      "Top documents (sources):\n",
      "[0] source=squad_paragraph_232\n",
      "[1] source=squad_paragraph_360\n",
      "[2] source=squad_paragraph_360\n",
      "[3] source=squad_paragraph_232\n",
      "[4] source=squad_paragraph_232\n",
      "--------------------------------------------------------------------------------\n",
      "Compressed context (preview):\n",
      "Chinese supporters have accused Western media of bias in their coverage, particularly regarding the torch relays. The Chinese ambassador to the UK, Fu Ying, claimed that Western media \"demonises\" China. Chinese netizens have criticized Western media for being biased, while Western reporters have described Chinese media as partial and censored. Demonstrators have expressed their discontent with Western media, carrying signs that read \"Shame on some Western media\" and \"Stop media distortion!\" Additionally, there have been protests by Chinese Australians in Sydney supporting Beijing and opposing perceived Western media bias.\n",
      "--------------------------------------------------------------------------------\n",
      "Ground truth answer: biased.\n",
      "--------------------------------------------------------------------------------\n",
      "Model answer:\n",
      "Chinese supporters have accused Western media of bias in their coverage.\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation scores:\n",
      "  correctness:  5\n",
      "  groundedness: 5\n",
      "  relevance:    5\n",
      "  explanation:  The model answer accurately reflects the context provided, stating that Chinese supporters have accused Western media of bias, which is directly supported by the context. It is also focused on the question asked.\n",
      "--------------------------------------------------------------------------------\n",
      "Latency (ms):\n",
      "  rewrite_ms: 856\n",
      "  retrieval_plus_rerank_ms: 6241\n",
      "  compression_ms: 2754\n",
      "  generation_ms: 1054\n",
      "  evaluation_ms: 2134\n",
      "  total_ms: 13041\n",
      "================================================================================\n",
      "================================================================================\n",
      "Original query: Which record label was the soundtrack album released on?\n",
      "Rewritten query: Which record label was the soundtrack album distributed by?\n",
      "--------------------------------------------------------------------------------\n",
      "Top documents (sources):\n",
      "[0] source=squad_paragraph_467\n",
      "[1] source=squad_paragraph_79\n",
      "[2] source=squad_paragraph_298\n",
      "[3] source=squad_paragraph_332\n",
      "[4] source=squad_paragraph_237\n",
      "--------------------------------------------------------------------------------\n",
      "Compressed context (preview):\n",
      "The soundtrack album was distributed by Decca Records, released on 23 October 2015 in the UK and 6 November 2015 in the USA.\n",
      "--------------------------------------------------------------------------------\n",
      "Ground truth answer: Decca Records\n",
      "--------------------------------------------------------------------------------\n",
      "Model answer:\n",
      "The soundtrack album was released on Decca Records.\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation scores:\n",
      "  correctness:  5\n",
      "  groundedness: 5\n",
      "  relevance:    5\n",
      "  explanation:  The model answer is factually correct as it states that the soundtrack album was released on Decca Records, which is directly supported by the context provided. Additionally, the answer is focused on the question asked.\n",
      "--------------------------------------------------------------------------------\n",
      "Latency (ms):\n",
      "  rewrite_ms: 1427\n",
      "  retrieval_plus_rerank_ms: 4225\n",
      "  compression_ms: 1082\n",
      "  generation_ms: 628\n",
      "  evaluation_ms: 1943\n",
      "  total_ms: 9306\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>correct</th>\n",
       "      <th>grounded</th>\n",
       "      <th>relevance</th>\n",
       "      <th>total_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who was the superior of prince Kublai?</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In turn, Chinese supporters have accused Weste...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>13041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which record label was the soundtrack album re...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  correct  grounded  \\\n",
       "0             Who was the superior of prince Kublai?        5         5   \n",
       "1  In turn, Chinese supporters have accused Weste...        5         5   \n",
       "2  Which record label was the soundtrack album re...        5         5   \n",
       "\n",
       "   relevance  total_ms  \n",
       "0          5     12230  \n",
       "1          5     13041  \n",
       "2          5      9306  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "if DATA_MODE == \"squad\":\n",
    "    sample_df = qa_df.sample(3, random_state=42)  # pick 3 for quick test\n",
    "    for _, row in sample_df.iterrows():\n",
    "        q = row[\"question\"]\n",
    "        gt = row[\"answer\"]\n",
    "        out = run_single_query_pipeline(vectordb, q, ground_truth=gt, use_memory=True)\n",
    "        row_result = {\n",
    "            \"query\": q,\n",
    "            \"correct\": out[\"eval\"][\"correctness\"],\n",
    "            \"grounded\": out[\"eval\"][\"groundedness\"],\n",
    "            \"relevance\": out[\"eval\"][\"relevance\"],\n",
    "            \"total_ms\": out[\"timings\"][\"total_ms\"],\n",
    "        }\n",
    "        results.append(row_result)\n",
    "else:\n",
    "    # Example queries for your own docs\n",
    "    example_queries = [\n",
    "        \"What is the main purpose of this document?\",\n",
    "        \"Summarize the key steps in the process described.\",\n",
    "    ]\n",
    "    for q in example_queries:\n",
    "        out = run_single_query_pipeline(vectordb, q, ground_truth=None, use_memory=True)\n",
    "        row_result = {\n",
    "            \"query\": q,\n",
    "            \"correct\": out[\"eval\"][\"correctness\"],\n",
    "            \"grounded\": out[\"eval\"][\"groundedness\"],\n",
    "            \"relevance\": out[\"eval\"][\"relevance\"],\n",
    "            \"total_ms\": out[\"timings\"][\"total_ms\"],\n",
    "        }\n",
    "        results.append(row_result)\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "110c1dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feedback for query:\n",
      "Who was the superior of prince Kublai?\n",
      "Correction: User correction: the correct answer is that the passage describes XYZ, not ABC.\n",
      "\n",
      "--- Re-running pipeline with feedback memory ---\n",
      "\n",
      "================================================================================\n",
      "Original query: Who was the superior of prince Kublai?\n",
      "Rewritten query: Who was the superior or higher authority of Prince Kublai Khan?\n",
      "--------------------------------------------------------------------------------\n",
      "Top documents (sources):\n",
      "[0] source=squad_paragraph_105\n",
      "[1] source=squad_paragraph_105\n",
      "[2] source=squad_paragraph_105\n",
      "[3] source=squad_paragraph_170\n",
      "[4] source=squad_paragraph_129\n",
      "--------------------------------------------------------------------------------\n",
      "Compressed context (preview):\n",
      "Prince Kublai Khan's superior authority was Ögedei Khan, who granted him a large appanage in North China starting in 1236. Kublai later ruled as Khagan from 1260 to 1294. Additionally, Kublai recognized the Phagpa lama as a senior instructor in religious affairs, establishing a unique relationship where Kublai was seen as the superior sovereign in political matters.\n",
      "--------------------------------------------------------------------------------\n",
      "Model answer:\n",
      "The superior of Prince Kublai was Ögedei Khan. This is stated in the context: \"Prince Kublai Khan's superior authority was Ögedei Khan.\"\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation scores:\n",
      "  correctness:  5\n",
      "  groundedness: 5\n",
      "  relevance:    5\n",
      "  explanation:  The model answer correctly identifies Ögedei Khan as the superior of Prince Kublai, which is explicitly stated in the context. It also directly addresses the question without introducing irrelevant information.\n",
      "--------------------------------------------------------------------------------\n",
      "Latency (ms):\n",
      "  rewrite_ms: 1178\n",
      "  retrieval_plus_rerank_ms: 4546\n",
      "  compression_ms: 2239\n",
      "  generation_ms: 1534\n",
      "  evaluation_ms: 3382\n",
      "  total_ms: 12881\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Example: pick one query and simulate a user correction\n",
    "\n",
    "if results:\n",
    "    example_query = results[0][\"query\"]\n",
    "    simulated_correction = \"User correction: the correct answer is that the passage describes XYZ, not ABC.\"\n",
    "\n",
    "    print(\"Adding feedback for query:\")\n",
    "    print(example_query)\n",
    "    print(\"Correction:\", simulated_correction)\n",
    "\n",
    "    memory_store.add_feedback(example_query, simulated_correction)\n",
    "\n",
    "    # Run again; memory is now part of retrieval\n",
    "    print(\"\\n--- Re-running pipeline with feedback memory ---\\n\")\n",
    "    out_after = run_single_query_pipeline(vectordb, example_query, ground_truth=None, use_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e021395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Original query: During which season of American Idol did Fox beat the other networks in ratings for the first time? \n",
      "Rewritten query: In which season of American Idol did the Fox network achieve higher television ratings than its competitors for the first time?\n",
      "--------------------------------------------------------------------------------\n",
      "Top documents (sources):\n",
      "[0] source=squad_paragraph_149\n",
      "[1] source=squad_paragraph_405\n",
      "[2] source=squad_paragraph_322\n",
      "[3] source=squad_paragraph_149\n",
      "[4] source=squad_paragraph_322\n",
      "--------------------------------------------------------------------------------\n",
      "Compressed context (preview):\n",
      "Fox achieved higher television ratings than its competitors for the first time during the seventh season of American Idol.\n",
      "--------------------------------------------------------------------------------\n",
      "Ground truth answer: season seven\n",
      "--------------------------------------------------------------------------------\n",
      "Model answer:\n",
      "Fox beat the other networks in ratings for the first time during the seventh season of American Idol.\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation scores:\n",
      "  correctness:  5\n",
      "  groundedness: 5\n",
      "  relevance:    5\n",
      "  explanation:  The model answer is factually correct as it matches the ground truth answer and the context provided. It directly addresses the question about which season Fox first beat other networks in ratings, confirming it was the seventh season of American Idol.\n",
      "--------------------------------------------------------------------------------\n",
      "Latency (ms):\n",
      "  rewrite_ms: 1000\n",
      "  retrieval_plus_rerank_ms: 3624\n",
      "  compression_ms: 1807\n",
      "  generation_ms: 1334\n",
      "  evaluation_ms: 2150\n",
      "  total_ms: 9918\n",
      "================================================================================\n",
      "================================================================================\n",
      "Original query: During which season of American Idol did Fox beat the other networks in ratings for the first time? \n",
      "Rewritten query: In which season of American Idol did the Fox network achieve higher television ratings than its competitors for the first time?\n",
      "--------------------------------------------------------------------------------\n",
      "Top documents (sources):\n",
      "[0] source=squad_paragraph_405\n",
      "[1] source=squad_paragraph_149\n",
      "[2] source=squad_paragraph_64\n",
      "[3] source=squad_paragraph_286\n",
      "[4] source=squad_paragraph_322\n",
      "--------------------------------------------------------------------------------\n",
      "Compressed context (preview):\n",
      "Fox achieved higher television ratings than its competitors for the first time during the seventh season of American Idol. This season's finale, featuring the competition between contestants David Cook and David Archuleta, marked a significant ratings rebound and helped Fox become the most watched TV network in the country, a first for a non-Big Three major broadcast network.\n",
      "--------------------------------------------------------------------------------\n",
      "Ground truth answer: season seven\n",
      "--------------------------------------------------------------------------------\n",
      "Model answer:\n",
      "Fox beat the other networks in ratings for the first time during the seventh season of American Idol. This is stated in the context: \"Fox achieved higher television ratings than its competitors for the first time during the seventh season of American Idol.\"\n",
      "--------------------------------------------------------------------------------\n",
      "Evaluation scores:\n",
      "  correctness:  5\n",
      "  groundedness: 5\n",
      "  relevance:    5\n",
      "  explanation:  The model answer is factually correct as it accurately states that Fox beat the other networks in ratings for the first time during the seventh season of American Idol, which is directly supported by the context provided.\n",
      "--------------------------------------------------------------------------------\n",
      "Latency (ms):\n",
      "  rewrite_ms: 916\n",
      "  retrieval_plus_rerank_ms: 8295\n",
      "  compression_ms: 1944\n",
      "  generation_ms: 1655\n",
      "  evaluation_ms: 2166\n",
      "  total_ms: 14979\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>correct</th>\n",
       "      <th>grounded</th>\n",
       "      <th>relevance</th>\n",
       "      <th>total_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>800</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  variant  chunk_size  correct  grounded  relevance  total_ms\n",
       "0       A         300        5         5          5      9918\n",
       "1       B         800        5         5          5     14979"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_vectorstore_with_chunk_size(chunk_size: int) -> FAISS:\n",
    "    vectordb_tmp, _ = build_vectorstore(base_docs, chunk_size=chunk_size, chunk_overlap=int(chunk_size * 0.2))\n",
    "    return vectordb_tmp\n",
    "\n",
    "def compare_chunk_sizes(\n",
    "    query: str,\n",
    "    gt_answer: Optional[str] = None,\n",
    "    chunk_sizes: Tuple[int, int] = (300, 800)\n",
    ") -> pd.DataFrame:\n",
    "    size_a, size_b = chunk_sizes\n",
    "\n",
    "    db_a = build_vectorstore_with_chunk_size(size_a)\n",
    "    db_b = build_vectorstore_with_chunk_size(size_b)\n",
    "\n",
    "    out_a = run_single_query_pipeline(db_a, query, ground_truth=gt_answer, use_memory=False)\n",
    "    out_b = run_single_query_pipeline(db_b, query, ground_truth=gt_answer, use_memory=False)\n",
    "\n",
    "    rows = []\n",
    "    for label, cs, out in [(\"A\", size_a, out_a), (\"B\", size_b, out_b)]:\n",
    "        rows.append({\n",
    "            \"variant\": label,\n",
    "            \"chunk_size\": cs,\n",
    "            \"correct\": out[\"eval\"][\"correctness\"],\n",
    "            \"grounded\": out[\"eval\"][\"groundedness\"],\n",
    "            \"relevance\": out[\"eval\"][\"relevance\"],\n",
    "            \"total_ms\": out[\"timings\"][\"total_ms\"],\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows), out_a, out_b\n",
    "\n",
    "\n",
    "# Example for SQuAD mode\n",
    "if DATA_MODE == \"squad\":\n",
    "    row = qa_df.sample(1, random_state=123).iloc[0]\n",
    "    q = row[\"question\"]\n",
    "    gt = row[\"answer\"]\n",
    "\n",
    "    cs_df, out_a, out_b = compare_chunk_sizes(q, gt_answer=gt, chunk_sizes=(300, 800))\n",
    "    display(cs_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Advanced-RAG-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
