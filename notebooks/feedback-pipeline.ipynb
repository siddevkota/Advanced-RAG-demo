{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2417e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ebpearls1/Desktop/Advanced-RAG-demo/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Set OPENAI_API_KEY in environment or .env file.\")\n",
    "\n",
    "# Configuration\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Pipeline parameters\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 80\n",
    "STAGE1_K = 30\n",
    "TOP_K_RERANKED = 5\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa1eec",
   "metadata": {},
   "source": [
    "## Initialize Reranker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3697738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANKER_MODEL_NAME = \"BAAI/bge-reranker-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_NAME)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def cross_encoder_rerank(\n",
    "    query: str,\n",
    "    docs: List[Document],\n",
    "    top_k: int = TOP_K_RERANKED\n",
    ") -> List[Document]:\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs = [(query, d.page_content) for d in docs]\n",
    "    inputs = tokenizer(\n",
    "        [p[0] for p in pairs],\n",
    "        [p[1] for p in pairs],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze(-1).cpu().numpy()\n",
    "\n",
    "    ranked_idx = np.argsort(-scores)\n",
    "    top_docs = [docs[i] for i in ranked_idx[:top_k]]\n",
    "    return top_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd69bc",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e45772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_model() -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_llm(model_name: str = LLM_MODEL, temperature: float = 0.7) -> ChatOpenAI:\n",
    "    return ChatOpenAI(\n",
    "        model=model_name,\n",
    "        temperature=temperature,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c483ad8",
   "metadata": {},
   "source": [
    "## Data Loading: SQuAD or PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd734614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_squad_subset(max_examples: int = 1000) -> Tuple[List[Document], pd.DataFrame]:\n",
    "    \"\"\"Load SQuAD dataset for testing.\"\"\"\n",
    "    ds = load_dataset(\"squad\", split=\"train[:10%]\")\n",
    "    ds = ds.shuffle(seed=42).select(range(min(max_examples, len(ds))))\n",
    "\n",
    "    contexts = []\n",
    "    qa_rows = []\n",
    "\n",
    "    for ex in ds:\n",
    "        context = ex[\"context\"]\n",
    "        q = ex[\"question\"]\n",
    "        ans_texts = ex[\"answers\"][\"text\"]\n",
    "        ans = ans_texts[0] if ans_texts else \"\"\n",
    "\n",
    "        contexts.append(context)\n",
    "        qa_rows.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"context\": context,\n",
    "            \"question\": q,\n",
    "            \"answer\": ans\n",
    "        })\n",
    "\n",
    "    unique_contexts = list({c: True for c in contexts}.keys())\n",
    "    docs = [Document(page_content=c, metadata={\"source\": f\"squad_paragraph_{i}\"})\n",
    "            for i, c in enumerate(unique_contexts)]\n",
    "\n",
    "    qa_df = pd.DataFrame(qa_rows)\n",
    "    return docs, qa_df\n",
    "\n",
    "def load_pdf_documents(pdf_dir: str = \"../data/pdfs\") -> List[Document]:\n",
    "    \"\"\"Load PDF documents from a directory.\"\"\"\n",
    "    pdf_path = Path(pdf_dir)\n",
    "    \n",
    "    if not pdf_path.exists():\n",
    "        print(f\"Creating directory: {pdf_dir}\")\n",
    "        pdf_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Please add PDF files to {pdf_dir} and run again.\")\n",
    "        return []\n",
    "    \n",
    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_dir}\")\n",
    "        return []\n",
    "    \n",
    "    docs = []\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Loading: {pdf_file.name}\")\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        docs.extend(loader.load())\n",
    "    \n",
    "    print(f\"Loaded {len(docs)} pages from {len(pdf_files)} PDF files\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246f197",
   "metadata": {},
   "source": [
    "## Build Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f020d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore(\n",
    "    docs: List[Document],\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> Tuple[FAISS, List[Document]]:\n",
    "    \"\"\"Build vectorstore with chunking.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    embeddings = get_embedding_model()\n",
    "    vectordb = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return vectordb, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b48a5",
   "metadata": {},
   "source": [
    "## Feedback Memory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c8b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackMemory:\n",
    "    \"\"\"Stores user feedback and preferences to improve future responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, save_path: str = \"../data/feedback_memory.json\"):\n",
    "        self.save_path = save_path\n",
    "        self.feedback_history: List[Dict[str, Any]] = []\n",
    "        self.preference_patterns: Dict[str, Any] = {\n",
    "            \"preferred_styles\": [],\n",
    "            \"preferred_lengths\": [],\n",
    "            \"preferred_structures\": [],\n",
    "            \"context_preferences\": []\n",
    "        }\n",
    "        self.load_memory()\n",
    "    \n",
    "    def add_feedback(\n",
    "        self,\n",
    "        query: str,\n",
    "        response_a: str,\n",
    "        response_b: str,\n",
    "        preferred: str,\n",
    "        context: str,\n",
    "        reason: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"Record user's preference between two responses.\"\"\"\n",
    "        feedback_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"query\": query,\n",
    "            \"response_a\": response_a,\n",
    "            \"response_b\": response_b,\n",
    "            \"preferred\": preferred,  # \"A\" or \"B\"\n",
    "            \"context\": context,\n",
    "            \"reason\": reason,\n",
    "            \"response_a_length\": len(response_a.split()),\n",
    "            \"response_b_length\": len(response_b.split())\n",
    "        }\n",
    "        \n",
    "        self.feedback_history.append(feedback_entry)\n",
    "        self._update_preference_patterns(feedback_entry)\n",
    "        self.save_memory()\n",
    "    \n",
    "    def _update_preference_patterns(self, feedback: Dict[str, Any]):\n",
    "        \"\"\"Analyze feedback to identify user preferences.\"\"\"\n",
    "        preferred_response = feedback[\"response_a\"] if feedback[\"preferred\"] == \"A\" else feedback[\"response_b\"]\n",
    "        preferred_length = feedback[\"response_a_length\"] if feedback[\"preferred\"] == \"A\" else feedback[\"response_b_length\"]\n",
    "        \n",
    "        self.preference_patterns[\"preferred_lengths\"].append(preferred_length)\n",
    "        \n",
    "        # Analyze style preferences\n",
    "        if \"detailed\" in feedback.get(\"reason\", \"\").lower() or len(preferred_response.split()) > 100:\n",
    "            self.preference_patterns[\"preferred_styles\"].append(\"detailed\")\n",
    "        elif \"concise\" in feedback.get(\"reason\", \"\").lower() or len(preferred_response.split()) < 50:\n",
    "            self.preference_patterns[\"preferred_styles\"].append(\"concise\")\n",
    "    \n",
    "    def get_preference_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of user preferences.\"\"\"\n",
    "        if not self.feedback_history:\n",
    "            return {\"total_feedback\": 0, \"insights\": \"No feedback yet\"}\n",
    "        \n",
    "        avg_length = np.mean(self.preference_patterns[\"preferred_lengths\"]) if self.preference_patterns[\"preferred_lengths\"] else 0\n",
    "        \n",
    "        style_counter = {}\n",
    "        for style in self.preference_patterns[\"preferred_styles\"]:\n",
    "            style_counter[style] = style_counter.get(style, 0) + 1\n",
    "        \n",
    "        preferred_style = max(style_counter.items(), key=lambda x: x[1])[0] if style_counter else \"balanced\"\n",
    "        \n",
    "        return {\n",
    "            \"total_feedback\": len(self.feedback_history),\n",
    "            \"avg_preferred_length\": int(avg_length),\n",
    "            \"preferred_style\": preferred_style,\n",
    "            \"style_distribution\": style_counter\n",
    "        }\n",
    "    \n",
    "    def get_generation_guidance(self) -> str:\n",
    "        \"\"\"Generate instruction for LLM based on learned preferences.\"\"\"\n",
    "        summary = self.get_preference_summary()\n",
    "        \n",
    "        if summary[\"total_feedback\"] == 0:\n",
    "            return \"\"\n",
    "        \n",
    "        guidance = \"Based on user preferences: \"\n",
    "        \n",
    "        if summary[\"avg_preferred_length\"] < 50:\n",
    "            guidance += \"Keep responses concise and to the point. \"\n",
    "        elif summary[\"avg_preferred_length\"] > 100:\n",
    "            guidance += \"Provide detailed, comprehensive responses. \"\n",
    "        \n",
    "        if summary[\"preferred_style\"] == \"detailed\":\n",
    "            guidance += \"Include explanations and background information. \"\n",
    "        elif summary[\"preferred_style\"] == \"concise\":\n",
    "            guidance += \"Focus on direct answers without extra elaboration. \"\n",
    "        \n",
    "        return guidance\n",
    "    \n",
    "    def save_memory(self):\n",
    "        \"\"\"Save feedback to disk.\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)\n",
    "        with open(self.save_path, 'w') as f:\n",
    "            json.dump({\n",
    "                \"feedback_history\": self.feedback_history,\n",
    "                \"preference_patterns\": self.preference_patterns\n",
    "            }, f, indent=2)\n",
    "    \n",
    "    def load_memory(self):\n",
    "        \"\"\"Load feedback from disk.\"\"\"\n",
    "        if os.path.exists(self.save_path):\n",
    "            with open(self.save_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                self.feedback_history = data.get(\"feedback_history\", [])\n",
    "                self.preference_patterns = data.get(\"preference_patterns\", self.preference_patterns)\n",
    "            print(f\"Loaded {len(self.feedback_history)} feedback entries from memory\")\n",
    "\n",
    "# Initialize feedback memory\n",
    "feedback_memory = FeedbackMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca581f",
   "metadata": {},
   "source": [
    "## RAG Pipeline with Dual Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c88c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank(\n",
    "    vectordb: FAISS,\n",
    "    query: str,\n",
    "    stage1_k: int = STAGE1_K,\n",
    "    top_k: int = TOP_K_RERANKED\n",
    ") -> List[Document]:\n",
    "    \"\"\"Retrieve and rerank documents.\"\"\"\n",
    "    candidates = vectordb.similarity_search(query, k=stage1_k)\n",
    "    top_docs = cross_encoder_rerank(query, candidates, top_k=top_k)\n",
    "    return top_docs\n",
    "\n",
    "def generate_dual_responses(\n",
    "    query: str,\n",
    "    context_docs: List[Document],\n",
    "    feedback_memory: FeedbackMemory\n",
    ") -> Tuple[str, str, str]:\n",
    "    \"\"\"Generate two different responses for comparison.\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([d.page_content for d in context_docs])\n",
    "    \n",
    "    # Get learned preferences\n",
    "    preference_guidance = feedback_memory.get_generation_guidance()\n",
    "    \n",
    "    # Response A: More detailed and explanatory\n",
    "    llm_a = get_llm(temperature=0.7)\n",
    "    system_msg_a = (\n",
    "        \"You are a helpful assistant that provides detailed, comprehensive answers. \"\n",
    "        \"Use the context to give thorough explanations with examples and background information. \"\n",
    "        f\"{preference_guidance}\"\n",
    "    )\n",
    "    \n",
    "    prompt_a = f\"\"\"Context:\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a detailed, comprehensive answer based on the context:\"\"\"\n",
    "    \n",
    "    messages_a = [\n",
    "        {\"role\": \"system\", \"content\": system_msg_a},\n",
    "        {\"role\": \"user\", \"content\": prompt_a}\n",
    "    ]\n",
    "    response_a = llm_a.invoke(messages_a).content.strip()\n",
    "    \n",
    "    # Response B: More concise and direct\n",
    "    llm_b = get_llm(temperature=0.3)\n",
    "    system_msg_b = (\n",
    "        \"You are a helpful assistant that provides concise, direct answers. \"\n",
    "        \"Use the context to give clear, to-the-point responses without unnecessary elaboration. \"\n",
    "        f\"{preference_guidance}\"\n",
    "    )\n",
    "    \n",
    "    prompt_b = f\"\"\"Context:\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Provide a concise, direct answer based on the context:\"\"\"\n",
    "    \n",
    "    messages_b = [\n",
    "        {\"role\": \"system\", \"content\": system_msg_b},\n",
    "        {\"role\": \"user\", \"content\": prompt_b}\n",
    "    ]\n",
    "    response_b = llm_b.invoke(messages_b).content.strip()\n",
    "    \n",
    "    return response_a, response_b, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ceac4",
   "metadata": {},
   "source": [
    "## Interactive Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7a7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_responses(query: str, response_a: str, response_b: str):\n",
    "    \"\"\"Display both responses for comparison.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"RESPONSE A (Detailed):\")\n",
    "    print(\"-\"*80)\n",
    "    print(response_a)\n",
    "    print(f\"\\nLength: {len(response_a.split())} words\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"RESPONSE B (Concise):\")\n",
    "    print(\"-\"*80)\n",
    "    print(response_b)\n",
    "    print(f\"\\nLength: {len(response_b.split())} words\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def get_user_feedback() -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"Get user's preferred response.\"\"\"\n",
    "    while True:\n",
    "        choice = input(\"\\nWhich response do you prefer? (A/B): \").strip().upper()\n",
    "        if choice in ['A', 'B']:\n",
    "            reason = input(\"Why did you prefer this response? (optional): \").strip()\n",
    "            return choice, reason if reason else None\n",
    "        print(\"Please enter 'A' or 'B'\")\n",
    "\n",
    "def run_interactive_query(\n",
    "    vectordb: FAISS,\n",
    "    query: str,\n",
    "    feedback_memory: FeedbackMemory\n",
    "):\n",
    "    \"\"\"Run a single query with dual responses and feedback collection.\"\"\"\n",
    "    print(f\"\\nüîç Processing query: {query}\")\n",
    "    \n",
    "    # Retrieve documents\n",
    "    top_docs = retrieve_and_rerank(vectordb, query)\n",
    "    print(f\"‚úì Retrieved {len(top_docs)} relevant documents\")\n",
    "    \n",
    "    # Generate dual responses\n",
    "    print(\"‚úì Generating two response variants...\")\n",
    "    response_a, response_b, context = generate_dual_responses(query, top_docs, feedback_memory)\n",
    "    \n",
    "    # Display responses\n",
    "    display_responses(query, response_a, response_b)\n",
    "    \n",
    "    # Get feedback\n",
    "    preferred, reason = get_user_feedback()\n",
    "    \n",
    "    # Store feedback\n",
    "    feedback_memory.add_feedback(\n",
    "        query=query,\n",
    "        response_a=response_a,\n",
    "        response_b=response_b,\n",
    "        preferred=preferred,\n",
    "        context=context,\n",
    "        reason=reason\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Feedback recorded! The system will learn from your preference.\")\n",
    "    \n",
    "    # Show preference summary\n",
    "    summary = feedback_memory.get_preference_summary()\n",
    "    print(f\"\\nüìä Preference Summary: {summary['total_feedback']} feedback entries collected\")\n",
    "    if summary['total_feedback'] > 0:\n",
    "        print(f\"   - Preferred style: {summary['preferred_style']}\")\n",
    "        print(f\"   - Average preferred length: {summary['avg_preferred_length']} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d0f05d",
   "metadata": {},
   "source": [
    "## Load Data and Build Index\n",
    "\n",
    "Choose your data source: SQuAD dataset or PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5428b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data source: squad\n",
      "Loaded 496 documents from SQuAD\n",
      "\n",
      "Building vectorstore...\n",
      "Loaded 496 documents from SQuAD\n",
      "\n",
      "Building vectorstore...\n",
      "‚úì Vectorstore built with 1272 chunks\n",
      "‚úì Vectorstore built with 1272 chunks\n"
     ]
    }
   ],
   "source": [
    "# Configuration: Choose data source\n",
    "DATA_SOURCE = \"squad\"  # Options: \"squad\" or \"pdf\"\n",
    "PDF_DIRECTORY = \"../data/pdfs\"  # Directory containing PDF files\n",
    "\n",
    "print(f\"Data source: {DATA_SOURCE}\")\n",
    "\n",
    "if DATA_SOURCE == \"squad\":\n",
    "    # Load SQuAD dataset\n",
    "    base_docs, qa_df = load_squad_subset(max_examples=600)\n",
    "    print(f\"Loaded {len(base_docs)} documents from SQuAD\")\n",
    "    \n",
    "elif DATA_SOURCE == \"pdf\":\n",
    "    # Load PDF documents\n",
    "    base_docs = load_pdf_documents(PDF_DIRECTORY)\n",
    "    if not base_docs:\n",
    "        print(\"\\n‚ö†Ô∏è  No PDF documents found. Please add PDFs to the directory.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(base_docs)} pages from PDF files\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"DATA_SOURCE must be 'squad' or 'pdf'\")\n",
    "\n",
    "# Build vectorstore\n",
    "if base_docs:\n",
    "    print(\"\\nBuilding vectorstore...\")\n",
    "    vectordb, chunks = build_vectorstore(base_docs)\n",
    "    print(f\"‚úì Vectorstore built with {len(chunks)} chunks\")\n",
    "else:\n",
    "    vectordb = None\n",
    "    print(\"‚ö†Ô∏è  No documents to index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb61ec4",
   "metadata": {},
   "source": [
    "## Run Interactive Pipeline\n",
    "\n",
    "Test the pipeline with example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ba47eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example queries available:\n",
      "1. What is the capital of France?\n",
      "2. Who invented the telephone?\n",
      "3. When did World War II end?\n",
      "\n",
      "You can use these or enter your own queries below.\n"
     ]
    }
   ],
   "source": [
    "# Example queries (modify based on your data)\n",
    "if DATA_SOURCE == \"squad\":\n",
    "    example_queries = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who invented the telephone?\",\n",
    "        \"When did World War II end?\"\n",
    "    ]\n",
    "else:\n",
    "    example_queries = [\n",
    "        \"What is the main topic of this document?\",\n",
    "        \"Summarize the key findings.\",\n",
    "        \"What are the main recommendations?\"\n",
    "    ]\n",
    "\n",
    "print(\"Example queries available:\")\n",
    "for i, q in enumerate(example_queries, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "print(\"\\nYou can use these or enter your own queries below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325bb5f",
   "metadata": {},
   "source": [
    "### Query 1: First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a02c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing query: What is the capital of France?\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "\n",
      "================================================================================\n",
      "QUERY: What is the capital of France?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "The capital of France is Paris. Paris is not only the political capital of the country but also serves as a major cultural, economic, and historical hub in Europe and the world. \n",
      "\n",
      "As mentioned in the context, Paris is significant in various industries, including finance, retail, tourism, and the arts. It is home to many iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral, which attract millions of tourists each year. The city's influence extends beyond its borders, as it is often considered a center for fashion and art, hosting events like Paris Fashion Week and various art exhibitions.\n",
      "\n",
      "In terms of cinema, the context references that Paris has a notable position in the film industry, with the second-highest ticket sales of all time for a movie, amounting to 4.1 million tickets sold, surpassed only by \"Spider-Man 3.\" This highlights the city's vibrant entertainment sector and its role in global cinema.\n",
      "\n",
      "Historically, Paris has been a focal point for cultural and intellectual movements, especially during the Enlightenment and the 19th century, when it became a hub for artists, writers, and philosophers.\n",
      "\n",
      "In summary, Paris is the capital of France, renowned for its rich history, cultural significance, and economic impact, making it one of the most important cities in the world.\n",
      "\n",
      "Length: 215 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "The capital of France is Paris.\n",
      "\n",
      "Length: 6 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUERY: What is the capital of France?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "The capital of France is Paris. Paris is not only the political capital of the country but also serves as a major cultural, economic, and historical hub in Europe and the world. \n",
      "\n",
      "As mentioned in the context, Paris is significant in various industries, including finance, retail, tourism, and the arts. It is home to many iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral, which attract millions of tourists each year. The city's influence extends beyond its borders, as it is often considered a center for fashion and art, hosting events like Paris Fashion Week and various art exhibitions.\n",
      "\n",
      "In terms of cinema, the context references that Paris has a notable position in the film industry, with the second-highest ticket sales of all time for a movie, amounting to 4.1 million tickets sold, surpassed only by \"Spider-Man 3.\" This highlights the city's vibrant entertainment sector and its role in global cinema.\n",
      "\n",
      "Historically, Paris has been a focal point for cultural and intellectual movements, especially during the Enlightenment and the 19th century, when it became a hub for artists, writers, and philosophers.\n",
      "\n",
      "In summary, Paris is the capital of France, renowned for its rich history, cultural significance, and economic impact, making it one of the most important cities in the world.\n",
      "\n",
      "Length: 215 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "The capital of France is Paris.\n",
      "\n",
      "Length: 6 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 1 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 6 words\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 1 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 6 words\n"
     ]
    }
   ],
   "source": [
    "if vectordb is not None:\n",
    "    # Use first example query or modify it\n",
    "    query1 = example_queries[0]\n",
    "    run_interactive_query(vectordb, query1, feedback_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91c348",
   "metadata": {},
   "source": [
    "### Query 2: Second Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f72ef134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing query: Who invented the telephone?\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "\n",
      "================================================================================\n",
      "QUERY: Who invented the telephone?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not mention the invention of the telephone. However, the telephone is commonly attributed to Alexander Graham Bell, who was awarded the first US patent for an \"improvement in telegraphy\" in 1876, which allowed for the transmission of voice over wires. Bell's invention marked a significant advancement in communication technology, enabling real-time voice conversations over long distances.\n",
      "\n",
      "While the context references telecommunications and devices inspired by existing designs, it does not directly address the history or inventor of the telephone. For a comprehensive understanding, it's essential to recognize that Bell's invention laid the groundwork for modern telecommunications, which have since evolved into the complex mobile and internet systems mentioned in the context.\n",
      "\n",
      "Length: 115 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "The context does not provide information about who invented the telephone.\n",
      "\n",
      "Length: 11 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUERY: Who invented the telephone?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not mention the invention of the telephone. However, the telephone is commonly attributed to Alexander Graham Bell, who was awarded the first US patent for an \"improvement in telegraphy\" in 1876, which allowed for the transmission of voice over wires. Bell's invention marked a significant advancement in communication technology, enabling real-time voice conversations over long distances.\n",
      "\n",
      "While the context references telecommunications and devices inspired by existing designs, it does not directly address the history or inventor of the telephone. For a comprehensive understanding, it's essential to recognize that Bell's invention laid the groundwork for modern telecommunications, which have since evolved into the complex mobile and internet systems mentioned in the context.\n",
      "\n",
      "Length: 115 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "The context does not provide information about who invented the telephone.\n",
      "\n",
      "Length: 11 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 2 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 8 words\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 2 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 8 words\n"
     ]
    }
   ],
   "source": [
    "if vectordb is not None:\n",
    "    # Use second example query or modify it\n",
    "    query2 = example_queries[1]\n",
    "    run_interactive_query(vectordb, query2, feedback_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5915f3",
   "metadata": {},
   "source": [
    "### Query 3: Third Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a517079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing query: When did World War II end?\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "\n",
      "================================================================================\n",
      "QUERY: When did World War II end?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "World War II ended on September 2, 1945, when Japan formally surrendered. This followed the earlier surrender of Germany on May 7, 1945, which marked the end of the war in Europe. The conclusion of World War II had significant global implications, including the establishment of new political orders, the onset of the Cold War, and the emergence of the United States as a dominant economic power, as indicated by the context provided. The post-war period saw substantial economic growth, influenced in part by returning veterans and government policies aimed at stimulating the economy.\n",
      "\n",
      "Length: 94 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "World War II ended in 1945.\n",
      "\n",
      "Length: 6 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUERY: When did World War II end?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "World War II ended on September 2, 1945, when Japan formally surrendered. This followed the earlier surrender of Germany on May 7, 1945, which marked the end of the war in Europe. The conclusion of World War II had significant global implications, including the establishment of new political orders, the onset of the Cold War, and the emergence of the United States as a dominant economic power, as indicated by the context provided. The post-war period saw substantial economic growth, influenced in part by returning veterans and government policies aimed at stimulating the economy.\n",
      "\n",
      "Length: 94 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "World War II ended in 1945.\n",
      "\n",
      "Length: 6 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 3 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 37 words\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 3 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 37 words\n"
     ]
    }
   ],
   "source": [
    "if vectordb is not None:\n",
    "    # Use third example query or modify it\n",
    "    query3 = example_queries[2]\n",
    "    run_interactive_query(vectordb, query3, feedback_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4fa58",
   "metadata": {},
   "source": [
    "### Custom Query\n",
    "\n",
    "Enter your own query below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de90504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing query: Where is Normandy?\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "‚úì Retrieved 5 relevant documents\n",
      "‚úì Generating two response variants...\n",
      "\n",
      "================================================================================\n",
      "QUERY: Where is Normandy?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not contain information about Normandy. However, I can provide a detailed answer about its location.\n",
      "\n",
      "Normandy is a region located in the northwestern part of France. It is bordered by the English Channel to the north, the regions of Bretagne (Brittany) to the west, Pays de la Loire to the southwest, Centre-Val de Loire to the southeast, and √éle-de-France to the east. Normandy is known for its rich history, including its role in the D-Day landings during World War II, and is famous for landmarks such as the picturesque Mont Saint-Michel and the historic city of Rouen.\n",
      "\n",
      "If you have more specific questions about Normandy or want to know how it relates to a particular topic or context, feel free to ask!\n",
      "\n",
      "Length: 126 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "The context does not provide information about Normandy.\n",
      "\n",
      "Length: 8 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUERY: Where is Normandy?\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE A (Detailed):\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not contain information about Normandy. However, I can provide a detailed answer about its location.\n",
      "\n",
      "Normandy is a region located in the northwestern part of France. It is bordered by the English Channel to the north, the regions of Bretagne (Brittany) to the west, Pays de la Loire to the southwest, Centre-Val de Loire to the southeast, and √éle-de-France to the east. Normandy is known for its rich history, including its role in the D-Day landings during World War II, and is famous for landmarks such as the picturesque Mont Saint-Michel and the historic city of Rouen.\n",
      "\n",
      "If you have more specific questions about Normandy or want to know how it relates to a particular topic or context, feel free to ask!\n",
      "\n",
      "Length: 126 words\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE B (Concise):\n",
      "--------------------------------------------------------------------------------\n",
      "The context does not provide information about Normandy.\n",
      "\n",
      "Length: 8 words\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 4 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 29 words\n",
      "\n",
      "‚úì Feedback recorded! The system will learn from your preference.\n",
      "\n",
      "üìä Preference Summary: 4 feedback entries collected\n",
      "   - Preferred style: concise\n",
      "   - Average preferred length: 29 words\n"
     ]
    }
   ],
   "source": [
    "if vectordb is not None:\n",
    "    # Enter your custom query here\n",
    "    custom_query = \"Where is Normandy?\"\n",
    "    \n",
    "    # Uncomment the line below to run with your custom query\n",
    "    run_interactive_query(vectordb, custom_query, feedback_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f0f56",
   "metadata": {},
   "source": [
    "## Analyze Feedback Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ce421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Feedback Analysis\n",
      "================================================================================\n",
      "\n",
      "Preference Distribution:\n",
      "  Response A (Detailed): 1 times\n",
      "  Response B (Concise): 3 times\n",
      "\n",
      "Average Response Lengths:\n",
      "  Response A: 137.5 words\n",
      "  Response B: 7.8 words\n",
      "\n",
      "Preferred Response Characteristics:\n",
      "  Average length: 29.8 words\n",
      "  Length range: 6 - 94 words\n",
      "\n",
      "üìù Recent Feedback (last 5):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: What is the capital of France?...\n",
      "Preferred: Response B\n",
      "Reason: It's a direct and short answer to my direct question.\n",
      "\n",
      "Query: Who invented the telephone?...\n",
      "Preferred: Response B\n",
      "Reason: The context should be used.\n",
      "\n",
      "Query: When did World War II end?...\n",
      "Preferred: Response A\n",
      "Reason: It explains the history which I prefer\n",
      "\n",
      "Query: Where is Normandy?...\n",
      "Preferred: Response B\n",
      "Reason: Context matters\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>query</th>\n",
       "      <th>preferred</th>\n",
       "      <th>response_a_length</th>\n",
       "      <th>response_b_length</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-20T17:10:41.483012</td>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>B</td>\n",
       "      <td>215</td>\n",
       "      <td>6</td>\n",
       "      <td>It's a direct and short answer to my direct qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-20T17:11:38.867262</td>\n",
       "      <td>Who invented the telephone?</td>\n",
       "      <td>B</td>\n",
       "      <td>115</td>\n",
       "      <td>11</td>\n",
       "      <td>The context should be used.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-20T17:12:23.777813</td>\n",
       "      <td>When did World War II end?</td>\n",
       "      <td>A</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>It explains the history which I prefer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-20T17:13:24.682253</td>\n",
       "      <td>Where is Normandy?</td>\n",
       "      <td>B</td>\n",
       "      <td>126</td>\n",
       "      <td>8</td>\n",
       "      <td>Context matters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp                           query preferred  \\\n",
       "0  2025-11-20T17:10:41.483012  What is the capital of France?         B   \n",
       "1  2025-11-20T17:11:38.867262     Who invented the telephone?         B   \n",
       "2  2025-11-20T17:12:23.777813      When did World War II end?         A   \n",
       "3  2025-11-20T17:13:24.682253              Where is Normandy?         B   \n",
       "\n",
       "   response_a_length  response_b_length  \\\n",
       "0                215                  6   \n",
       "1                115                 11   \n",
       "2                 94                  6   \n",
       "3                126                  8   \n",
       "\n",
       "                                              reason  \n",
       "0  It's a direct and short answer to my direct qu...  \n",
       "1                        The context should be used.  \n",
       "2             It explains the history which I prefer  \n",
       "3                                    Context matters  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View all feedback history\n",
    "if feedback_memory.feedback_history:\n",
    "    feedback_df = pd.DataFrame(feedback_memory.feedback_history)\n",
    "    \n",
    "    print(\"\\nüìä Feedback Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Preference distribution\n",
    "    pref_counts = feedback_df['preferred'].value_counts()\n",
    "    print(f\"\\nPreference Distribution:\")\n",
    "    print(f\"  Response A (Detailed): {pref_counts.get('A', 0)} times\")\n",
    "    print(f\"  Response B (Concise): {pref_counts.get('B', 0)} times\")\n",
    "    \n",
    "    # Average lengths\n",
    "    print(f\"\\nAverage Response Lengths:\")\n",
    "    print(f\"  Response A: {feedback_df['response_a_length'].mean():.1f} words\")\n",
    "    print(f\"  Response B: {feedback_df['response_b_length'].mean():.1f} words\")\n",
    "    \n",
    "    # Preferred lengths\n",
    "    preferred_a_lengths = feedback_df[feedback_df['preferred'] == 'A']['response_a_length']\n",
    "    preferred_b_lengths = feedback_df[feedback_df['preferred'] == 'B']['response_b_length']\n",
    "    all_preferred_lengths = pd.concat([preferred_a_lengths, preferred_b_lengths])\n",
    "    \n",
    "    if len(all_preferred_lengths) > 0:\n",
    "        print(f\"\\nPreferred Response Characteristics:\")\n",
    "        print(f\"  Average length: {all_preferred_lengths.mean():.1f} words\")\n",
    "        print(f\"  Length range: {all_preferred_lengths.min():.0f} - {all_preferred_lengths.max():.0f} words\")\n",
    "    \n",
    "    # Show recent feedback\n",
    "    print(f\"\\nüìù Recent Feedback (last 5):\")\n",
    "    print(\"-\"*80)\n",
    "    for entry in feedback_memory.feedback_history[-5:]:\n",
    "        print(f\"\\nQuery: {entry['query'][:60]}...\")\n",
    "        print(f\"Preferred: Response {entry['preferred']}\")\n",
    "        if entry.get('reason'):\n",
    "            print(f\"Reason: {entry['reason']}\")\n",
    "    \n",
    "    # Display dataframe\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    display(feedback_df[['timestamp', 'query', 'preferred', 'response_a_length', 'response_b_length', 'reason']].tail(10))\n",
    "    \n",
    "else:\n",
    "    print(\"No feedback collected yet. Run some queries first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a3e43",
   "metadata": {},
   "source": [
    "## View Learned Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c433499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Learned User Preferences\n",
      "================================================================================\n",
      "Total feedback collected: 4\n",
      "\n",
      "Preferred Style: CONCISE\n",
      "Average Preferred Length: 29 words\n",
      "\n",
      "Style Distribution:\n",
      "  Concise: 3 times\n",
      "\n",
      "üìã Current Generation Guidance:\n",
      "  Based on user preferences: Keep responses concise and to the point. Focus on direct answers without extra elaboration. \n",
      "\n",
      "‚ú® The system will use these preferences to improve future responses!\n"
     ]
    }
   ],
   "source": [
    "summary = feedback_memory.get_preference_summary()\n",
    "guidance = feedback_memory.get_generation_guidance()\n",
    "\n",
    "print(\"\\nüéØ Learned User Preferences\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total feedback collected: {summary['total_feedback']}\")\n",
    "\n",
    "if summary['total_feedback'] > 0:\n",
    "    print(f\"\\nPreferred Style: {summary['preferred_style'].upper()}\")\n",
    "    print(f\"Average Preferred Length: {summary['avg_preferred_length']} words\")\n",
    "    \n",
    "    if summary.get('style_distribution'):\n",
    "        print(f\"\\nStyle Distribution:\")\n",
    "        for style, count in summary['style_distribution'].items():\n",
    "            print(f\"  {style.capitalize()}: {count} times\")\n",
    "    \n",
    "    print(f\"\\nüìã Current Generation Guidance:\")\n",
    "    print(f\"  {guidance}\")\n",
    "    \n",
    "    print(\"\\n‚ú® The system will use these preferences to improve future responses!\")\n",
    "else:\n",
    "    print(\"\\nNo preferences learned yet. Provide feedback to help the system learn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecbf12",
   "metadata": {},
   "source": [
    "## Export Feedback Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64bb7355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Feedback data exported to: ../data/feedback_export.csv\n",
      "  Total entries: 4\n"
     ]
    }
   ],
   "source": [
    "# Export feedback to CSV for further analysis\n",
    "if feedback_memory.feedback_history:\n",
    "    output_path = \"../data/feedback_export.csv\"\n",
    "    feedback_df = pd.DataFrame(feedback_memory.feedback_history)\n",
    "    feedback_df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úì Feedback data exported to: {output_path}\")\n",
    "    print(f\"  Total entries: {len(feedback_df)}\")\n",
    "else:\n",
    "    print(\"No feedback to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811017e4",
   "metadata": {},
   "source": [
    "## Continuous Query Loop (Optional)\n",
    "\n",
    "Run this cell to enter a continuous loop for multiple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9eb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_continuous_loop(vectordb: FAISS, feedback_memory: FeedbackMemory):\n",
    "    \"\"\"Run continuous query loop until user stops.\"\"\"\n",
    "    print(\"\\nüîÑ Continuous Query Mode\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Enter 'quit' or 'exit' to stop\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nEnter your question: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\n‚úì Exiting continuous mode\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            print(\"Please enter a valid question\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            run_interactive_query(vectordb, query, feedback_memory)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚úì Interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error: {e}\")\n",
    "            continue\n",
    "\n",
    "# Uncomment to run continuous loop\n",
    "# if vectordb is not None:\n",
    "#     run_continuous_loop(vectordb, feedback_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ec7e3a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements:\n",
    "1. ‚úÖ Dual data source support (SQuAD + PDF)\n",
    "2. ‚úÖ Dual response generation (detailed vs concise)\n",
    "3. ‚úÖ User feedback collection\n",
    "4. ‚úÖ Preference learning and adaptation\n",
    "5. ‚úÖ Memory-based improvement over time\n",
    "\n",
    "The system learns from your preferences and adapts future responses accordingly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Advanced-RAG-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
